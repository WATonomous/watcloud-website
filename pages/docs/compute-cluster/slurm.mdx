# SLURM

SLURM (Simple Linux Utility for Resource Management)[^slurm] is an open-source job scheduler that handles the allocation of resources in a compute cluster.
It is commonly used in HPC (High Performance Computing) environments. WATcloud uses SLURM to manage most of its compute resources.

[^slurm]: https://slurm.schedmd.com/


## Quick Start

import { Callout } from 'nextra/components'

<Callout type="info">WATcloud SLURM is currently in beta. If you encounter any issues, please [let us know](/docs/compute-cluster/support-resources).</Callout>

To submit jobs to the SLURM cluster, you will need to log into one of the SLURM login nodes.
During the beta, they are labelled `SL` in the [machine list](/machines).
After the beta, all general-use machines will be SLURM login nodes.

### Interactive shell

Execute the following command to submit a job to the SLURM cluster:

```bash
srun --pty bash
```

This will start an interactive shell session on a compute node with the default resources.
You can view the resources allocated to your job by running:

```bash
scontrol show job $SLURM_JOB_ID
```

An example output is shown below:

```text {6,17}
JobId=1305 JobName=bash
   UserId=ben(1507) GroupId=ben(1507) MCS_label=N/A
   Priority=1 Nice=0 Account=watonomous-watcloud QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=0:0
   RunTime=00:00:04 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2024-03-16T06:39:57 EligibleTime=2024-03-16T06:39:57
   AccrueTime=Unknown
   StartTime=2024-03-16T06:39:57 EndTime=2024-03-16T07:09:57 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-03-16T06:39:57 Scheduler=Main
   Partition=compute AllocNode:Sid=10.1.100.128:1060621
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=wato2-slurm1
   BatchHost=wato2-slurm1
   NumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=1,mem=512M,node=1,billing=1,gres/tmpdisk=100
   AllocTRES=cpu=1,mem=512M,node=1,billing=1,gres/tmpdisk=100
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryCPU=512M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=bash
   WorkDir=/home/ben
   Power=
   TresPerNode=gres/tmpdisk:100
```

In this example, the job is allocated 1 CPU, 512MiB of memory, and 100MiB of temporary disk space (mounted at `/tmp`),
and is allowed to run for up to 30 minutes.

To request for more resources, you can use the `--cpus-per-task`, `--mem`, `--gres`, and `--time` flags.
For example, to request 4 CPUs, 4GiB of memory, 20GiB of temporary disk space, and 2 hours of runtime, you can run:

```bash
srun --cpus-per-task 4 --mem 4G --gres tmpdisk:20480 --time 2:00:00 --pty bash
```

### Using Docker

Unlike general use machines, the SLURM environment does not provide user-space systemd for managing background processes like the Docker daemon.
To use Docker, you will need to start the Docker daemon manually. We have provided a convenience script to do this:

```bash
slurm-start-dockerd.sh
```

If successful, you should see the following output:

```text
Dockerd started successfully! Execute the following command to use it:

export DOCKER_HOST=unix:///tmp/run/docker.sock
docker run --rm hello-world
```

Follow the instructions to set the `DOCKER_HOST` environment variable accordingly.

### Using GPUs

You can request access to GPUs by using the `--gres mps` flag. For example, if your workload requires 4 GiB of VRAM, you can run:

```bash
srun --gres mps:4096 --pty bash
```

This will allocate GPU resources to your job using the NVIDIA Multi-Process Service (MPS)[^mps].
Using MPS is the preferred way to request for GPU resources because it allows multiple jobs to
share the same GPU.

[^mps]: https://docs.nvidia.com/deploy/mps/index.html

If your workload requires exclusive access to a GPU, you can use the `--gres gpu` flag instead:

<Callout type="warning">Because the cluster is GPU-constrained, requesting whole GPUs is not recommended unless your workload can
make efficient use of the entire GPU.</Callout>

```bash
srun --gres gpu:1 --pty bash
```

This will allocate a whole GPU to your job. Note that this will prevent other jobs from using the GPU until your job is finished.

### Using CUDA

If your workload requires CUDA, you have a few options (not exhaustive):

#### Using the `nvidia/cuda` Docker image

You can use the `nvidia/cuda` Docker image to run CUDA workloads.
Assuming you have started the Docker daemon (see [Using Docker](#using-docker)), you can run the following command to start a CUDA container:

```bash
docker run --rm -it --gpus all -v $(pwd):/workspace nvidia/cuda:12.0.0-devel-ubuntu22.04 nvcc --version
```

Note that the version of the Docker image must be compatible (usually this means lower than or equal to) with the driver version installed on the compute node.
You can check the driver version by running `nvidia-smi`. If the driver version is not compatible with the Docker image, you will get an error that looks like this:

```text
> docker run --rm -it --gpus all -v $(pwd):/workspace nvidia/cuda:12.1.0-runtime-ubuntu22.04
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'
nvidia-container-cli: requirement error: unsatisfied condition: cuda>=12.1, please update your driver to a newer version, or use an earlier cuda container: unknown.
```

#### Using the Compute Canada CUDA module

The Compute Canada CVMFS[^cc-cvmfs] is mounted on the compute nodes. You can access CUDA by loading the appropriate module:

```bash
# Set up the module environment
source /cvmfs/soft.computecanada.ca/config/profile/bash.sh
# Load the appropriate environment
module load StdEnv/2023
# Load the CUDA module
module load cuda/12.2
# Check the nvcc version
nvcc --version
```

Compute Canada only provides select versions of CUDA, and does not provide an easy way to list all available versions.
A trick you can use is to run `which nvcc{:bash}` and trace back along the directory tree to find sibling directories
that contain other CUDA versions.

Note that the version of CUDA must be compatible with the driver version installed on the compute node.
You can check the driver version by running `nvidia-smi`.
You can find the CUDA compatibility matrix [here](https://docs.nvidia.com/deploy/cuda-compatibility/index.html).

[^cc-cvmfs]: The [Compute Canada CVMFS](https://docs.alliancecan.ca/wiki/Accessing_CVMFS) is mounted at `/cvmfs/soft.computecanada.ca` on the compute nodes. It provides access to a wide variety of software via [Lmod modules](https://docs.alliancecan.ca/wiki/Utiliser_des_modules/en).

## Extra details

### SLURM v.s. general-use machines

The SLURM environment is configured to be as close to the general-use environment as possible.
All of the same network drives and software are available. However, there are some differences:

- The SLURM environment uses a `/tmp` drive for temporary storage instead of `/mnt/scratch` on general-use machines.
    Temporary storage can be requested using the `--gres tmpdisk:<size_in_MiB>` flag.
- The SLURM environment does not have a user-space systemd for managing background processes like the Docker daemon.
    Please follow the instructions in the [Using Docker](#using-docker) section to start the Docker daemon.

### `gres/tmpdisk`

`tmpdisk` is a generic resource (GRES)[^gres] that represents temporary disk space. This resource is provisioned using a combination
of `job_container/tmpfs`[^job-container-tmpfs] and custom scripts. The temporary disk space is mounted at `/tmp` and is automatically
cleaned up when the job finishes. You can request for temporary disk space using the `--gres tmpdisk:<size_in_MiB>` flag.

[^gres]: https://slurm.schedmd.com/gres.html
[^job-container-tmpfs]: https://slurm.schedmd.com/job_container_tmpfs.html

### `gres/mps` and `gres/gpu`

`mps` and `gpu` are GRES that represent GPU resources.
Allocation of these resources is managed by built-in SLURM plugins that interface with various GPU libraries.

The `mps` GRES is used to request access to a portion of a GPU[^mps-management].
In the WATcloud cluster, the amount of allocable `mps` equals the amount of VRAM (in MiB) on each GPU.
This representation is chosen because it is a concrete metric that is the same across different GPU models.
In reality, the amount of `mps` represents the share of the GPU's *compute* resources that the job can use,
and the VRAM usage is not enforced. Please ensure that the mps requested is appropriate for your workload.
To request for `mps`, use the `--gres mps:<size_in_MiB>` flag.

[^mps-management]: For more information on MPS, please refer to the [MPS Management](https://slurm.schedmd.com/gres.html#MPS_Management) SLURM documentation.

The `gpu` GRES is used to request exclusive access to GPUs[^gpu-management].
This is not recommended unless your workload can make efficient use of the entire GPU.
If you are unsure, please use the `mps` GRES instead.
To request for `gpu`, use the `--gres gpu:<number_of_gpus>` flag.

[^gpu-management]: For more information on GPU management, please refer to the [GPU Management](https://slurm.schedmd.com/gres.html#GPU_Management) SLURM documentation.

### CVMFS

CVMFS (CernVM File System)[^cvmfs] is a software distribution system that is widely adopted in the HPC community.
It provides a way to distribute software to compute nodes without having to install them on the nodes themselves.

We make use of the [Compute Canada CVMFS](https://docs.alliancecan.ca/wiki/Accessing_CVMFS) to provide access to software available on Compute Canada clusters.
For example, you can access CUDA by loading the appropriate module (see [Using CUDA](#using-cuda)).

[^cvmfs]: https://cvmfs.readthedocs.io/en/stable/

{
// Separate footnotes from the main content
}
import { Separator } from "@/components/ui/separator"

<Separator className="mt-6" />