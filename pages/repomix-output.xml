This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
blog/
  _meta.json
  hello-world.mdx
  using-slurm-to-run-github-actions.mdx
  what-is-watcloud.mdx
docs/
  community-docs/
    watcloud/
      development-manual.mdx
      guidelines.mdx
      kubernetes-cheat-sheet.mdx
      maintenance-manual.mdx
      observability.mdx
      proxmox.mdx
      user-requests.mdx
    _meta.json
    watcloud.mdx
  compute-cluster/
    _meta.json
    firewall.mdx
    getting-access.mdx
    machine-usage-guide.mdx
    overview.mdx
    quotas.mdx
    slurm.mdx
    ssh.mdx
    support-resources.mdx
  utilities/
    _meta.json
    assets.mdx
    github.mdx
    onboarding-form.mdx
    profile-editor.mdx
  _meta.json
  community-docs.mdx
  compute-cluster.mdx
  index.mdx
  registered-affiliations.mdx
  services.mdx
get-involved/
  _meta.json
  index.mdx
  join.mdx
  sponsor.mdx
_app.mdx
_document.tsx
_meta.json
blog.mdx
index.mdx
machines.mdx
onboarding-form.mdx
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="blog/_meta.json">
{
    "*": {
        "display": "hidden",
        "theme": {
            "toc": false,
            "sidebar": false,
            "pagination": false,
            "typesetting": "article"
        }
    }
}
</file>

<file path="blog/hello-world.mdx">
---
title: Hello, World!
description: A test blog post to see if this works.
date: 2023-11-02
timezone: America/Vancouver
authors:
  - ben
reviewers: []
notify_subscribers: false
hidden: true
---

import { BlogPostHeader, BlogPostFooter } from '@/components/blog-post'

<BlogPostHeader />

This is a test blog post. Click [here](/machines) to see a list of machines.

<BlogPostFooter />
</file>

<file path="blog/using-slurm-to-run-github-actions.mdx">
---
title: Using Slurm to Run GitHub Actions
description: How we transitioned our provisioning pipeline to run in our Slurm Cluster. Includes the architecture, challenges, and performance improvements achieved.
title_image:
  square: 'blog-slurm-ci-square'
  wide: 'blog-slurm-ci-wide'
  attribution: |
    Image generated using DALL-E with ChatGPT-assisted prompt: "A comic-style illustration of a futuristic data center where GitHub Actions workflows are transformed into Slurm-powered jobs. Rows of towering server racks with glowing cables and cooling fans line both sides of the scene. In the center, a conveyor belt moves GitHub Actions runner capsules toward a massive Slurm control server, which processes them with robotic arms, spinning gears, and streams of digital energy. On the other side, the capsules emerge as fully optimized compute nodes, ready for execution. A small Octocat in a lab coat and safety goggles stands nearby, observing the process with curiosity. The scene is illuminated by a cool blue and purple glow, emphasizing the high-tech and automated nature of the system."
date: 2025-04-06
timezone: America/Toronto
authors:
  - alexboden
reviewers:
  - ben
notify_subscribers: true
hidden: false 
---

import { Cards } from 'nextra/components'
import Picture from '@/components/picture'
import {
  BlogSlurmCiGraphDark,
  BlogSlurmCiGraphLight,
  BlogSlurmCiPipelineDark,
  BlogSlurmCiPipelineLight,
} from '@/build/fixtures/images'


## Problem Statement
At WATcloud, we partition our compute resources into two primary pools: a small pool (\~60 vCPUs and \~120 GiB RAM) dedicated to a Kubernetes cluster, and a larger pool (\~240 vCPUs and \~700 GiB RAM) allocated to a Slurm cluster.

The Kubernetes cluster, managed by WATcloud administrators, hosts essential infrastructure including monitoring services, API backends, and the Slurm database and control servers. The Slurm cluster is available for use by cluster users through the open-source job scheduling system, [Slurm](https://slurm.schedmd.com/documentation.html).

Our Continuous Integration and Continuous Delivery (CI/CD) pipeline, responsible for provisioning compute resources and managing user workflows, runs on the Kubernetes cluster using GitHub Actions. This setup leverages the popular Kubernetes Operator, [actions-runner-controller (ARC)](https://github.com/actions/actions-runner-controller), maintained by GitHub.
<br />
<div className="flex justify-center items-center flex-col">
	<span className="hidden dark:block">
		<Picture alt="Slurm CI Pipeline (Dark)" image={BlogSlurmCiPipelineDark}  style={{ backgroundColor: 'transparent', backgroundImage: 'none', background: 'none'}} />
	</span>
	<span className="dark:hidden">
		<Picture alt="Slurm CI Pipeline (Light)" image={BlogSlurmCiPipelineLight} style={{ backgroundColor: 'transparent', backgroundImage: 'none', background: 'none'}} />
	</span>
	_Our current CI/CD pipeline_
</div>

The ARC performed reliably after the initial setup. However, as our pipeline grew, we found our Kubernetes cluster lacked sufficient resources to fully utilize potential parallelism, particularly when running multiple pipelines concurrently. Additionally, during periods without active jobs, resources remained idle and underutilized. While Kubernetes auto-scaling could address these challenges in cloud environments like AWS, GCP, or Azure, our infrastructure runs on bare-metal hardware, where resource allocation is fixed.

Considering this, we explored alternatives and identified Slurm—a dedicated job scheduling system—as potentially more suitable than Kubernetes for managing GitHub Actions workflows. Since ARC primarily functions as a job scheduler, Slurm aligns more naturally with our use case. With Slurm, we can leverage the significantly larger resource pool in our existing cluster, maximizing peak capacity during busy periods. Moreover, resources remain fully available for other workloads during idle periods, enhancing overall efficiency. Additionally, adopting Slurm simplifies resource accounting, allowing us to manage CI/CD workloads alongside other tenant workloads seamlessly, streamlining our overall resource management strategy.

| Computer Type      | Resources                                  |
|--------------------|--------------------------------------------|
| Kubernetes Cluster | Limited Dedicated Resources (~120 GiB RAM)  |
| Slurm Cluster      | Large Shared Resources (~700 GiB RAM)      |

The Slurm cluster offers significantly more resources, which allows us to have a higher peak throughput. Additionally, we could repurpose Kubernetes resources for Slurm if we no longer needed the actions-runner-controller.

## Solution
We opted to run our CI jobs on the Slurm cluster. After looking at available options, such as [github.com/ripley-cloud/gha-slurm](https://github.com/ripley-cloud/gha-slurm), we found them overly complex, often requiring multiple separately deployed modules.

This led us to implement our own solution which can be found [here](https://github.com/WATonomous/slurm-gha). In this blog post, we will explore the solution's architecture, challenges encountered, and performance improvements achieved.

### Solution Architecture
1. A Python script polls the GitHub API for queued jobs.
2. When a job is detected, an ephemeral action runner is allocated on the Slurm cluster.
3. Upon job completion, the runner and Slurm resources are released.

### Basic diagram of the system
```mermaid
flowchart TD
	subgraph "Kubernetes Cluster"
		ActionsRunners[("slurm-gha")]
		
	end
    GitHubAPI[("GitHub API")]
    Slurm[("Slurm Compute Resources")]

    ActionsRunners --> | Poll Queued Jobs | GitHubAPI 
    ActionsRunners -->| Allocate Actions Runner| Slurm 
```

## Minimum Viable Product (MVP)
After implementing an MVP of the script, we were able to run our CI jobs on the Slurm cluster. The biggest problem we encountered was that, unlike how Kubernetes nodes provide long-term image cache, we had to pull the runner image for every job. This added nearly two minutes to each job's execution time due to duplicate image pulls, which caused network congestion. Given that workflows contain up to 70 jobs, the additional time quickly became significant. This meant the implementation using Slurm was slower than the ARC.

## Speeding up the startup time of our Actions Runner Image
We explored caching the image manually on the filesystem but found it impractical due to the need to mount directories across multiple concurrent runners.

This led us to investigate several options:
- [Docker pull-through cache](https://docs.docker.com/docker-hub/mirror/)
- [Stargz Snapshotter](https://github.com/containerd/stargz-snapshotter)
- [Apptainer (formerly singularity)](https://apptainer.org/docs/user/main/index.html)

We tried all of the options, and below is a summary of our findings.

| Method | Description | Pros | Cons |
|--------|-------------|------|------|
| Docker pull-through cache | Relocates network traffic from the 1 Gbps uplink to the 10 Gbps cluster network. | Straightforward, directly compatible with Docker, relatively easy to set up | Pulled image still needs to be extracted (~20s) |
| Stargz snapshotter | Uses a special image format to allow starting the image without fully downloading it. | Quick startup time, doesn't require full image downloads | Difficult to set up, still prone to duplicate network traffic |
| Apptainer | Alternative to Docker. Allows starting images from unpacked layers. | Very quick startup time; when combined with CVMFS, provides file-level caching and deduplication | Difficult to set up, requires switching from Docker to Apptainer |

We ultimately chose the Apptainer approach. We already have CVMFS set up as a part of our SLURM cluster, and we found that CERN has a public CVMFS repository, [`unpacked.cern.ch`](https://indico.cern.ch/event/764570/contributions/3173502/attachments/1735975/2807816/CVMFS-unpacked.pdf), where anyone can submit images to and get them served as unpacked layers. They [kindly](https://github.com/cvmfs/images-unpacked.cern.ch/pull/29) hosted our runner image for us. 

With Apptainer set up, the runner image's startup time with the cached and unpacked image was much faster, and repeated access from different jobs is pretty much instant. Since we have a fixed number of Slurm nodes, this optimization reduced network traffic from 1 image fetch per job to 1 image fetch per node. It even improved compared to the original ARC setup, because CVMFS automatically does file-level deduplication, and runner image updates often don't change too much. This was a significant improvement for a pipeline executing 25,000 jobs per month, consuming 35,000 minutes of runtime.

## Speeding up our provisioner image pull
A second major challenge involved the repeated pulling of our provisioner image, a 1.5GB container used to [provision services](../docs/community-docs/watcloud/development-manual#terminology). This image differs from the Actions Runner image as it is rebuilt within each workflow, making caching trickier.

### Previous Solution
Originally, we created and uploaded the provisioner image to a self-hosted S3-compatible [RGW](https://docs.ceph.com/en/latest/man/8/radosgw/) store in each workflow run. When using the actions-runner-controller, this setup worked well, as each job only involves one network hop (both the job pod and the RGW pod run on Kubernetes). With Slurm, however, jobs now required at least one hop (from the Slurm node to the Kubernetes ingress), and sometimes two (in case the Kubernetes ingress pod is on a different Kubernetes node than the RGW pod). Going through the Kubernetes ingress turned out to be much slower than using the internal Kubernetes network.

### Inspiration from CERN
Initially, we looked to use the same Apptainer solution as we had for the runner image. However, this presented a new set of challenges, namely, that the CVMFS [DUCC](https://cvmfs.readthedocs.io/en/stable/cpt-ducc.html) pipeline is used to unpack the image for use in `unpacked.cern.ch`. Since the CERN pipeline runs hourly, we looked into running the DUCC command ourselves. However, the DUCC command builds multiple sets of layers to be compatible with software like Podman and ultimately took too long (around 2 minutes for the provisioner container) for our use case. 

### Our Solution : CVMFS Ephemeral Server
This led us to develop a [custom Docker unpack](https://github.com/WATonomous/docker-unpack) tool which only does a simple unpack. This reduced the unpacking time from minutes to around 10 seconds. We use this tool in our on-prem [CVMFS ephemeral server](https://github.com/WATonomous/cvmfs-ephemeral/). This is a CVMFS stratum 0 server meant for storing ephemeral data, like our provisioner image.

## Results
Comparing the actions-runner-controller to the Slurm implementation, we observed a significant reduction in runtime. We have 2 main workflow types: the main branch workflow which includes around 70 jobs, and the user ingestion precheck, which does a dry-run on a smaller set of jobs affecting user provisioning.

By leveraging Apptainer to reduce the frequency of image pulls, and adopting a CVMFS stratum 0 server for caching our provisioner images, we found **✨50%✨** reduction in time across both main branch and user ingestion workflows.

<br />

<div className="flex justify-center items-center">
	<span className="hidden dark:block">
		<Picture alt="Slurm CI Graph (Dark)" image={BlogSlurmCiGraphDark}  style={{ backgroundColor: 'transparent', backgroundImage: 'none', background: 'none'}} />
	</span>
	<span className="dark:hidden">
		<Picture alt="Slurm CI Graph (Light)" image={BlogSlurmCiGraphLight} style={{ backgroundColor: 'transparent', backgroundImage: 'none', background: 'none'}} />
	</span>
</div>

> **Note:** The actions-runner-controller (before) has higher variance as the resources can become saturated by concurrent workflows. The Slurm implementation (after) has a more consistent runtime as there are sufficient resources to achieve full parallelism.

## Conclusions

Migrating GitHub Actions workloads to the Slurm cluster provided a scalable and resource-efficient solution to the limitations of our existing Kubernetes setup. By leveraging our existing HPC infrastructure, including CVMFS, we are able to achieve higher parallelism, reduce idle costs, and consolidate resource management. This approach demonstrates how job schedulers like Slurm can effectively handle CI/CD workflows. In the future, we hope to further optimize our CI/CD pipeline through conditional job execution and more efficient resource planning to maximize developer velocity.

## Appendix

<details>
  <summary>Some useful tools WATcloud made for analyzing GitHub Actions performance</summary>

- [CI Runtime tool](https://github.com/WATonomous/ci-runtime) : Calculate the fastest theoretical runtime for a Github Actions workflow assuming infinite concurrency and no waiting between jobs.
- [GitHub Actions tracing](https://github.com/WATonomous/github-actions-tracing) : A tool for generating traces from GitHub Actions. The generated traces are compatible with Perfetto.

</details>

<details>
  <summary>Other implementation details</summary>

### Enabling Docker Within CI Jobs

We run our CI jobs with a container, [actions-runner-image](https://github.com/WATonomous/actions-runner-image). Within these jobs we also run containers, thus we needed to run Docker within Docker.

```mermaid
graph RL
    A[Docker Rootless Daemon] -->|Creates| B[Docker Rootless Socket]
    C[Actions Runner Image] -->|Mounts| B
    C -->|Creates| E[CI Helper Containers]
    E -->|Mounts| B
```

Normally it would be a security risk to mount the Docker socket into a container, but since we are using [Docker Rootless](https://docs.docker.com/engine/security/rootless/), we are able to mitigate this risk.

> **Note:** The CI's Docker commands will use the same filesystem, as they have the same Docker socket, you must configure the working directory of your runners accordingly. In our case this meant placing the working directory in ephemeral storage, via the `/tmp/` folder within a Slurm job.

### Deployment to Kubernetes

We deployed this on our self-hosted Kubernetes via a Docker image. To communicate with the GitHub API, an access token is needed. For our use case, a [personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#about-personal-access-tokens) which provides 5000 requests per hour was sufficient.

To enable communication with the Slurm controller, we set up a [munge key](https://dun.github.io/munge/). The Python script is then able to allocate an Actions Runner by triggering a bash script run with `sbatch`.

</details>
</file>

<file path="blog/what-is-watcloud.mdx">
---
title: 'Under the Hood: What is WATcloud?'
description: "Curious about how WATcloud makes computing resources easily and fairly accessible to everyone? Join us as we kick off a blog series exploring our four-year journey of refining our compute cluster's hardware and software—from servers and networking to user provisioning and observability systems."
title_image:
  square: 'under-the-hood-square'
  wide: 'under-the-hood-wide'
  attribution: |
    Image generated using DALL-E with ChatGPT-assisted prompt: "A comic-style drawing of a person standing in front of a larger car with a red hood, looking under the hood and finding it filled with computer parts instead of an engine. The car should have no engine visible; instead, the hood reveals computer parts like circuit boards, CPUs, cooling fans, and cables. The person is smaller in size compared to the car, standing on the ground, and looks surprised, with wide eyes and a raised eyebrow. The scene is light-hearted and humorous, with vibrant colors and exaggerated expressions, capturing the surprise and confusion of the person."
date: 2024-09-22
timezone: America/Vancouver
authors:
  - ben
reviewers:
  - j257jian
notify_subscribers: true
hidden: false
---

WATcloud's mission is to make computing resources easily and fairly accessible to everyone.
Over the past four years, we have continuously refined both the hardware and software that comprise our cluster.
On the hardware side, WATcloud includes servers, switches, IP-KVMs[^ip-kvm], and other peripherals that form our compute infrastructure.
On the software side, we have developed a variety of systems for managing the compute cluster, user provisioning, and auxiliary services.
These efforts encompass over 3,000 pull requests and tickets in our internal monorepo, along with more than 40 satellite repositories—ranging
from bug-fixing forks of open-source projects to custom tools.
Throughout the years, we've learned and grown immensely, and we're excited to share our journey with you.

[^ip-kvm]: IP-KVM stands for Internet Protocol Keyboard, Video, and Mouse. It allows us to remotely control servers (console access, power cycling, etc.) as if we were physically present.

Over the next few blog posts, we'll be sharing the architecture of WATcloud in its current state and how we got here.
Here's a list of topics we'll be covering:
- Compute cluster hardware setup
- Operating system, hypervisor, and virtual machine configuration
- Distributed storage configuration
- Network configuration
- Compute cluster provisioning pipeline
- User provisioning pipeline
- Resource management systems
- Observability systems
- On-prem and cloud-based Kubernetes clusters (and automatic failover)
- Continuous integration and deployment (CI/CD) configuration
- DNS management
- Email infrastructure
- Website/blog infrastructure
- Auxiliary services (Discord, GitHub, Google Workspace, etc.)
- Current major projects

This list may be adjusted as we publish more posts, and we'll make sure to hyperlink each item as we go.
This blog post serves as a preview for what's to come. If you'd like to stay up-to-date with our latest posts,
please subscribe to our newsletter below.

Do you have a suggestion for a blog post topic? Please let us know below in the comment section or reach out to us on [Discord](https://discord.gg/hEX5Q4KDYN).
</file>

<file path="docs/community-docs/watcloud/development-manual.mdx">
# WATcloud Development Manual

This document is a collection of snippets and notes that are useful for the development process.
It's not meant to be a complete guide, but rather a collection of useful information.
This is similar to our [internal notes](https://github.com/WATonomous/infra-notes), except that
all information here is non-sensitive. We will gradually migrate information from the internal notes to
this document.

This document is best read when cross-referenced with the code in the [infra-config](https://github.com/WATonomous/infra-config) repo.

## Terminology

We use the following terminology 

- **Infrastructure**: The set of all hosts and services that we manage.
- **Host**: A physical machine (bare-metal machine) or virtual machine (VM) that is managed by us. For example, our compute cluster is a set of hosts that are managed by us.
- **Cluster**: A set of hosts that are managed together. For example, our compute cluster is a set of hosts that are managed together.
- **User**: A person that is authorized to access our infrastructure. For example, a WATonomous member.
- **Service**: Anything that we provide to our users. The compute cluster, VMs, GPUs, the CI pipeline, the Kubernetes cluster, the GitHub organization, the Google Workspace, etc. are all services.
- **Directory**: The directory contains configurations for users and services. For example, `./directory/users` contains configurations for users and `./directory/hosts` contains configurations for hosts.
- **Provisioning**: The process of setting up a service. For example, setting up a VM.
- **Provisioner**: A tool that is used to provision a service. This can be low-level tools like Ansible or Terraform, or high-level tools like our GitHub provisioner and our Google Workspace provisioner.

## General Guidelines

- Read and understand the [WATcloud Guidelines](./guidelines) before starting development.
- Pull-request early and often. We have a lot of safe guards and automation in place to help you follow best practices.
  When CI is run, look out for automated comments on and pull requests against your PR!
- When writing commit messages and pull requests, start with a title that describes the change in imperative mood (e.g. "Add", "Fix", "Update")[^commit-message-convention],
  followed by more information in the body of the message. Use [linking keywords][linking-keywords] like `Resolves #<issue_number>` to automatically close issues.
  For example:

  ```markdown
  Create status-page Sentry project

  This commit introduces a new Sentry project for the [status page][sp].

  Resolves #123

  [sp]: https://github.com/WATonomous/status
  ```

[^commit-message-convention]: Derived from https://github.com/joelparkerhenderson/git-commit-message/blob/d5bcb65e263217bfe47d898c69f9c6c0dfd6d413/README.md

[linking-keywords]: https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue

## Getting Started

Many provisioners require access to the cluster network.
For simplicity, we will assume that you are using one of the [machines](/machines) in the cluster.

Clone the `infra-config` repo:

```bash copy
git clone git@github.com:WATonomous/infra-config.git
```

Start the development container. `git fetch` helps to check if the provisioner is up to date with master:

```bash copy
git fetch \
&& docker compose build provisioner \
&& docker compose run --rm provisioner /bin/bash
```

From now on, all commands should be run from within the container.

All provisioners in the `infra-config` repo have the same self-documenting interface:

```bash
./<provisioner>/provision.sh
# or 
./scripts/provision-<provisioner>.sh
```

For example, to run the GitHub provisioner:

```bash copy
./github/provision.sh # `github` is just an example, please replace it with the provisioner you are working with.
```

When you're done, please exit the container. The container (and stored secrets) will be destroyed automatically:

```bash copy
exit
```

## Secrets

We manage secrets using [Ansible Vault](https://docs.ansible.com/ansible/latest/user_guide/vault.html).
All commands below should be run inside the provisioner development environment (see [Getting Started](#getting-started) above).

### Authenticating with `ansible-vault`

Before performing any encrypt/decrypt actions, authenticate with `ansible-vault`:

```bash copy
./scripts/ansible-vault-authenticate.sh
```

### Encrypting secrets using `ansible-vault`

Add the output of the following command to `secrets/secrets.yml`:

```bash copy
printf "%s" 'super_s3cr3t_str1ng$$' | ./scripts/encrypt-secret.sh "name_of_secret"
```

### Decrypting secrets using `ansible-vault`

Example:

```bash copy
./scripts/decrypt-secret.sh ansible_ssh_pass
```

## Provisioner Container

The `provisioner` container is a development and provisioning environment with all the necessary tools installed.
It is defined in `docker-compose.yml` and can be started with the following command:

```bash copy
docker compose run --rm provisioner /bin/bash
```

This section describes properties of the container and describes changes that can be made to it.

### Read-Only Filesystem

The `infra-config` directory is mounted as read-only in the container, with a few exceptions as described in `docker-compose.yml`.
The read-only filesystem is useful for preventing accidental changes to the configuration during provisioning.
However, sometimes we want to make changes from within the container.
For example, we may want to allow a Terraform-based provisioner to update the dependency lock file.
To make the filesystem writable, we can make the following changes to `docker-compose.yml`:

```diff
diff --git a/docker-compose.yml b/docker-compose.yml
index 0641ded10f..5290b02478 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -3,7 +3,7 @@ services:
     build: .
     image: ${COMPOSE_PROJECT_NAME:?A COMPOSE_PROJECT_NAME is required to prevent container collision}
     volumes:
-      - .:/infra-config:ro
+      - .:/infra-config:rw
       # The output directory is mounted as rw so that the provisioner can write
       # to it.
       - ./outputs:/infra-config/outputs:rw
```

### Caching

We use GitHub Container Registry to cache the provisioner Docker image[^caching-details].
The cache is used to speed up both [CI](https://github.com/WATonomous/infra-config/pull/3170)
and [local development](https://github.com/WATonomous/infra-config/pull/3175).

The cache is automatically used. Without any changes, the following command should complete quickly[^build-time-with-cache]
and show that almost every stage is loaded from the cache:

```bash copy
docker compose build provisioner
```

Previously, there was a [cache invalidation issue](https://github.com/WATonomous/infra-config/pull/3176) when files in
the build context don't have the same permissions as the cache[^git-docker-permission-difference].
This issue has been fixed using a [workaround](https://github.com/WATonomous/infra-config/pull/3179).

[^caching-details]: [Here](https://github.com/WATonomous/infra-config/blob/121af9af1dbe78e187670163545fa6537a26757f/.github/workflows/push-images.yml#L62) is where we push the cache, and [here](https://github.com/WATonomous/infra-config/blob/121af9af1dbe78e187670163545fa6537a26757f/docker-compose.yml#L5-L6) is where we use it. The cache lives [here](https://github.com/WATonomous/infra-config/pkgs/container/infra-config).

[^build-time-with-cache]: At the time of writing (2024-09-16), the build time with cache is about 30 seconds on a single core (Docker immediately recognizes that every layer can be cached, and downloads the image from the cache). The build time without cache is about 3 minutes and 40 seconds on 8 cores.

[^git-docker-permission-difference]: Git and Docker handle file permissions differently. Git [only preserves the executable bit](https://stackoverflow.com/a/3211396/4527337) of files, and uses the umask ([defaults to `022`](https://man7.org/linux/man-pages/man2/umask.2.html) on most systems) to determine the permissions of the files it creates. Docker, on the other hand, [uses all permission bits](https://docs.docker.com/engine/reference/builder/#copy) when using `COPY` or `ADD`.


### Port forwarding

Sometimes, we may need to access services running in the container from the host machine.
For example, we may want to forward a Kubernetes service to the host machine for debugging.
To forward a port from the container to the host machine, we can make the following changes to `docker-compose.yml`:

```diff
diff --git a/docker-compose.yml b/docker-compose.yml
index 0641ded10f..42300307d2 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -13,6 +13,8 @@ services:
     working_dir: /infra-config
     environment:
       DRY_RUN: ${DRY_RUN:-}
+    ports:
+      - "1234:5678"
     dns:
       # In case the host's DNS is not working
       - 1.1.1.1
```

The above configuration forwards port `5678` in the container to port `1234` on the host machine.
Then we can run the following command to start the container:

```bash copy
docker compose run --rm --service-ports provisioner /bin/bash
```

Note that the `--service-ports` flag is required to forward the ports when using `docker compose run`[^docker-compose-run-service-ports].
Also note that the host port range is shared between all users on the machine, and the command above may silently fail if the port is already in use.
In that case, you can simply choose a different port.

Then, from within the container, we can start any service on port `5678` to have it accessible on the host machine on port `1234`.
For example, to start a simple HTTP server to serve files from the `outputs` directory:

```bash copy
npx serve -l 5678 ./outputs
```

Now, we can access the HTTP server from the host machine by visiting `http://localhost:1234` (e.g. `curl http://localhost:1234`).

As a practical example, we can forward the Prometheus service running in the Kubernetes cluster:

```bash copy
./kubernetes/run.sh kubectl port-forward -n prometheus --address 0.0.0.0 service/prometheus-kube-prometheus-prometheus 5678:9090
```

[^docker-compose-run-service-ports]: https://docs.docker.com/compose/reference/run/#options

## Ansible

### Developing Ansible Roles

Occasionally, we may need to develop new Ansible roles or fork existing ones to fix bugs or add features.
A list of existing roles can be found by [searching for `ansible-role-` on GitHub](https://github.com/WATonomous?q=ansible-role-&type=all&language=&sort=).

To develop a role, we can clone the role locally and mount it into our development environment.
For example, to develop [ansible-role-microk8s](https://github.com/WATonomous/ansible-role-microk8s),
we do the following:

```bash copy
# Clone the role alongside the infra-config repo
git clone git@github.com:WATonomous/infra-config.git
git clone git@github.com:WATonomous/ansible-role-microk8s.git
```

The resulting folder structure should look like this:

import { FileTree } from 'nextra/components'

<FileTree>
  <FileTree.Folder name="infra-config" defaultOpen>
    <FileTree.File name="docker-compose.yml" />
    <FileTree.File name="ansible-galaxy-requirements.yml" />
    <FileTree.File name="... other files" />
  </FileTree.Folder>
  <FileTree.Folder name="ansible-role-microk8s" />
</FileTree>

We will work in the `infra-config` directory:

```bash copy
cd infra-config
```

We can mount the role into our development environment by making the following changes to `docker-compose.yml`:

```ansi
[0;1mdiff --git a/docker-compose.yml b/docker-compose.yml[0m
[0;1mindex 30e581dab..cc9b34874 100644[0m
[0;1m--- a/docker-compose.yml[0m
[0;1m+++ b/docker-compose.yml[0m
[0;36m@@ -8,6 +8,7 @@[0m services:
       # The output directory is mounted as rw so that the provisioner can write
       # to it.
       - ./outputs:/infra-config/outputs:rw
[0;32m+      - ../ansible-role-microk8s:/root/.ansible/roles/watonomous.microk8s:ro[0m
     tmpfs:
       - /run:exec
       - /tmp:exec
```

Now, when we run the provisioner, it will use the local copy of the role instead of the one installed by Ansible Galaxy.

After we're done developing the role, we can remove the mount from `docker-compose.yml` and submit a PR to the role's repo. Once the PR is merged, we can update the role version in `ansible-galaxy-requirements.yml` and remove the local copy of the role.

For most custom roles, we simply use the commit hash as the version. Ansible Galaxy will automatically
download the role from GitHub using the commit hash. So updating the role version in `ansible-galaxy-requirements.yml` is
as simple as:

```ansi
[0;1mdiff --git a/ansible/ansible-galaxy-requirements.yml b/ansible/ansible-galaxy-requirements.yml[0m
[0;1mindex cdf321adf..b2ba4c1ec 100644[0m
[0;1m--- a/ansible/ansible-galaxy-requirements.yml[0m
[0;1m+++ b/ansible/ansible-galaxy-requirements.yml[0m
[0;36m@@ -24,7 +24,7 @@[0m roles:
     version: 7854b75566d7cb3f41009f83a3ceee93c2890262
   - name: watonomous.microk8s
     src: git+https://github.com/WATonomous/ansible-role-microk8s
[0;31m-    version: dfe4a5c92207d08462b6f206cd7b42010b34fa38[0m
[0;32m+    version: d544a16cbde82bd7457a8d498215ad78e6a689d0[0m
   - name: geerlingguy.filebeat
     src: git+https://github.com/geerlingguy/ansible-role-filebeat
     version: 407a4c3cd31cc8f9c485b9177fb7287e71745efb
```

## Terraform

### Developing Terraform Providers

We use Terraform providers extensively in our provisioners.
Sometimes, we may need to develop new Terraform providers or fork existing ones to fix bugs or add features.
This section describes how to develop Terraform providers.

We will use the [Discord Provider](https://github.com/WATonomous/terraform-provider-discord) as an example.

```bash copy
# Clone the provider alongside the infra-config repo
git clone git@github.com:WATonomous/infra-config.git
git clone git@github.com:WATonomous/terraform-provider-discord.git
```

The resulting folder structure should look like this:

<FileTree>
  <FileTree.Folder name="infra-config" defaultOpen>
    <FileTree.File name="docker-compose.yml" />
    <FileTree.File name="... other files" />
  </FileTree.Folder>
  <FileTree.Folder name="terraform-provider-discord" />
</FileTree>

Prepare two terminal windows. In one terminal, we will build the provider binary:

```bash copy
cd terraform-provider-discord
go build -o terraform-provider-discord
```

The above command creates a `terraform-provider-discord` binary that we will use later.

In another terminal, we will work in the `infra-config` directory:

```bash copy
cd infra-config
```

We can mount the role into our development environment by making the following changes to `docker-compose.yml`:

```diff
diff --git a/docker-compose.yml b/docker-compose.yml
index 7b282d9b45..62e684128c 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -7,6 +7,7 @@ services:
       # The output directory is mounted as rw so that the provisioner can write
       # to it.
       - ./outputs:/infra-config/outputs:rw
+      - ../terraform-provider-discord:/tf-dev/discord
     tmpfs:
       - /run:exec
       - /tmp:exec
```

Start the development container as usual:

```bash copy
git fetch \
&& docker compose build provisioner \
&& docker compose run --rm provisioner /bin/bash
```

In the container, create `~/.terraformrc` with the following content:

```ini filename="~/.terraformrc" {3} copy
provider_installation {
  dev_overrides {
    "terraform.local/local/discord" = "/tf-dev/discord"
  }

  filesystem_mirror {
    path    = "/usr/share/terraform/plugins"
    include = ["terraform.local/*/*"]
  }

  direct {
    exclude = ["terraform.local/*/*"]
  }
}
```

Note that `terraform.local/local/discord` is the provider's `source` in the `required_providers` block in the Terraform configuration
and `/tf-dev/discord` is the path we mounted the provider to.

The above configuration tells Terraform to search for a `/tf-dev/discord/terraform-provider-discord` binary when the `discord` provider is required,
instead of using a provider installed in `/usr/share/terraform/plugins` or downloaded from the registry.

Now, we can run the provisioner as usual. Terraform will use the local provider binary instead of the one installed in the container.

```bash copy
./discord/provision.sh
```

#### References
- https://discuss.hashicorp.com/t/development-overrides-for-providers-under-development/18888/2
- https://developer.hashicorp.com/terraform/cli/config/config-file#development-overrides-for-provider-developers
</file>

<file path="docs/community-docs/watcloud/guidelines.mdx">
# WATcloud Guidelines

WATcloud is a student-run organization.
The guaranteed turnover of team members as they graduate poses unique challenges in terms of
cluster maintenance and making progress.
Because of this, we've developed a set of guidelines to ensure that the knowledge of the team is
passed down effectively and that the quality of service is maintained.

This document outlines guidelines that every WATcloud team member should follow,
to maximize the learning of team members and to ensure the highest quality of service for users.

We divide our guidelines into two categories: technical—to address the maintainability of our systems,
and organizational—to encourage team members to maintain a fast-paced and productive environment amidst
internal and external bureaucratic challenges.

import { Steps } from 'nextra/components'

## Technical Guidelines

The central tenet of our technical guidelines is that everything we do must be trivially maintainable.
This means that with little guidance, a resourceful team member[^resourceful] should be able to understand
and maintain the systems we build and deploy, even if the original author is no longer with the team.

The following are a set of guidelines to strive for in our technical work:

[^resourceful]: A resourceful team member is one who is willing to learn and is able to find the information they need to solve a problem.
    Most of the time, this boils down to being good at Googling.

<Steps>
### Simplify everything

Complexity is the enemy of maintainability.
We should strive to make everything as simple as possible.
For example, we follow the principle of having a single source of truth[^ssot] by having a single [status page](https://status.watonomous.ca/)
that displays the status of all our services, and a single [provisioner interface](./development-manual#getting-started) for provisioning
different types of resources.

[^ssot]: Single source of truth (SSOT) is a principle that states that every piece of data should be stored in only one place.
    This reduces the likelihood of data inconsistencies and makes it easier to maintain the data.
    See [Wikipedia](https://en.wikipedia.org/wiki/Single_source_of_truth) for more information.

### Only host services that are necessary and easy to maintain

If we host something, it is inevitable that it will go down once in a while[^best-code-is-no-code].
We should only host services if the value they provide is worth the maintenance burden.

Some common red flags that a service is not worth hosting are:
- Static websites (use GitHub Pages)
- Services that have hosted alternatives (commercial services usually have free plans for open-source projects/non-profit organizations, e.g. Sentry, Elastic)
- Services that have unnecessary runtime complexity (e.g. services that call an API to retrieve information, when the information can be bundled with the service during building/deployment)

[^best-code-is-no-code]: As the saying goes, "The best code is no code at all."

### Version-control everything

All code, configuration, and documentation should be version-controlled.
Achieving 100% infrastructure-as-code[^iac] is impossible because we work with physical hardware, but we should strive for it as much as possible.
In cases where manual changes are necessary, they should be documented extensively.

[^iac]: IaC, [1](https://www.redhat.com/en/topics/automation/what-is-infrastructure-as-code-iac), [2](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/infrastructure-as-code)

### Make sure everything is reproducible

If a service goes down, we should be able to bring it back up with minimal effort.
This means that all dependencies should be documented (either as code or as documentation) and that the deployment process should be reasonably automated.

### Healthcheck everything

WATcloud has a lot of [observability](./observability) infrastructure in place to detect issues before they become problems.
Every service should be monitored, and alerts should be set up to notify us when something goes wrong.

### Communicate accurately

Communication, especially technical, should be accurate, as it can be very misleading if it is incorrect.
For example:
- `MB` (megabytes—$10^6$ bytes or $2^{20}$ bytes, depending on the context) is not the same as `Mb` (megabits—$10^6$ bits or $2^{20}$ bits, depending on the context) or `mb` (even more ambiguous, could be anything).
- Use `MiB` instead of `MB` whenever you know for sure that you are referring to the IEC[^iec] mebibytes ($2^{20}$ bytes) instead of the SI[^si] megabytes ($10^6$ bytes).

[^iec]: The International Electrotechnical Commission (IEC) defines binary prefixes "kibi", "mebi", "gibi", etc., to refer to powers of 2.
[^si]: The International System of Units (SI) defines prefixes "kilo", "mega", "giga", etc., to refer to powers of 10. However, in computing, these prefixes are often used to refer to powers of 2.

Brand/organization names should also be capitalized correctly. For example:
- Use `WATcloud` or `watcloud` instead of `WATCloud`, `WatCloud`, or `Watcloud`.
- Use `WATonomous` or `watonomous` instead of `Watonomous`.

</Steps>

## Organizational Guidelines

Student life is busy, and it can be difficult to balance the demands of school/work, the team, and personal life.
The following are a set of guidelines to help team members maintain a fast-paced and productive environment:

<Steps>


### Iterate quickly

Most of WATcloud's projects go through many iterations before they are sufficiently polished for production.
Embrace the concept of rapid prototyping.
Start with a minimum viable product (MVP) and iterate quickly and often based on feedback (from users, team members, or your future self).
This approach allows us to quickly identify what works and what doesn't, and to pivot as necessary.

### Proactively firefight

The term "fire" is a metaphor for an urgent issue that needs to be resolved immediately.
The term "firefighting" refers to the act of resolving these issues.
Firefighting is an unavoidable part of maintaining services like ones that WATcloud provides.
Some common causes of fires are:
- Power outages
- Hardware failures
- Software bugs

During development, follow the [technical guidelines](#technical-guidelines) to reduce the likelihood of fires.
When something goes wrong, it's important to act quickly to minimize the impact on users.
After issues are resolved, we should conduct a post-mortem to understand and document what went wrong and how we can prevent it from happening again.

### Minimize bureaucracy

Bureaucracy is the enemy of productivity.
At WATcloud, we strive to minimize bureaucracy as much as possible.
This means that we should avoid unnecessary hierarchies and hiding of information.

Sometimes, external bureaucracy is unavoidable.
For example, we have to follow the university's process for purchasing using the team's funds.
In these cases, we should try to be as efficient as possible to achieve our goals.
For example, oftentimes, we need to experiment with different products to find the best ones for our needs,
and potentially return the products that don't meet our requirements.
Instead of asking the university to purchase on our behalf, we can purchase the products ourselves and get reimbursed
for the products that we decide to keep.

### Be scrappy

Sometimes, we'll need to work within constraints that are not ideal.
In these cases, it's helpful to adopt a "hacker mentality" and find creative solutions to problems.
For example, since the beginning, a major constraint for WATcloud has been the budget.
Historically, instead of getting set-and-forget hardware that is expensive,
we worked very hard to minimize costs by opting for consumer-grade hardware and
using lots of healthchecks to ensure that the hardware is running as expected.

The same applies when your work depends on other people's work.
If you need a work-in-progress feature to be completed before you can start your work,
find a way to work on something else in the meantime.
It's almost always possible to divide work into smaller pieces that can be worked on independently.
Being blocked on someone else's work shouldn't be an excuse to stop working.

### Move fast and break things

At WATcloud, we embrace the motto "Move fast and break things" to encourage risk-taking and experimentation.
Making mistakes is a natural part of the learning process, and as long as we also move fast to fix what we break,
we can turn those mistakes into opportunities for improvement.
This philosophy complements our [technical guidelines](#technical-guidelines), which prioritize trivially maintainable systems.

With comprehensive observability, a dedication to infrastructure-as-code, and a commitment to minimize and document manual changes,
we are equipped to quickly identify and resolve issues as they arise.

### Establish Boundaries

WATcloud is committed to delivering high-quality, maintainable systems.
To ensure that everything we release is well-supported, we must consider the implications of each new feature.
Our capacity to [address urgent issues](#proactively-firefight) is not unlimited, thus prioritization is essential.
Often, our role is to provide a robust interface on which users can build.

For instance, our [SSH access instructions](/docs/compute-cluster/ssh) include reliable one-liner commands that work consistently.
However, configuration of SSH and agent forwarding—which vary based on the user's operating system and SSH agent—are left to the user.
For these more personalized setups, we direct users to the extensive resources available online.

</Steps>
</file>

<file path="docs/community-docs/watcloud/kubernetes-cheat-sheet.mdx">
import Picture from '@/components/picture'
import { DocKubernetesCheatSheetK9s } from '@/build/fixtures/images'

# Kubernetes Cheat Sheet

> **Note:**  This is an internal cheat sheet specific to WATcloud's Kubernetes set up.

## When using the provisioner

To run vanilla kubectl, `run.sh` will populate the `KUBECONFIG` env var so that the command knows which server to connect to and with what credentials

```bash
./kubernetes/run.sh kubectl
```

To access the azure kubernetes, simply use `./azure_kubernetes/run.sh`

### Some useful commands
List all pods in all namespaces (you can use `-A` or `--all-namespaces`)

```bash
./kubernetes/run.sh kubectl get pods -A
```

List all nodes in all namespaces

```bash
./kubernetes/run.sh kubectl get nodes -A
```

### k9s

If you want to use a nice graphical interface you can use `k9s`. This is useful for learning Kubernetes because it lists all the common commands.

```bash
./kubernetes/run.sh k9s -A
```

Sample output of `k9s`:
<Picture alt="k9s interface" image={DocKubernetesCheatSheetK9s} />

You can type `:` to open the command prompt and type `help` to see all the commands.

## Provisioning a project
To provision a project, you can use the `provision.sh` script. Where `folder_name` is in `infra-config/kubernetes/*` and contains the terraform config.

```bash
./kubernetes/provision.sh folder_name
```
</file>

<file path="docs/community-docs/watcloud/maintenance-manual.mdx">
# WATcloud Maintenance Manual

This manual outlines the maintenance procedures for various components of WATcloud.

## General procedure

1. **Plan the maintenance**: Prepare a plan for the maintenance, including the start time, end time, and the steps to be taken during the maintenance. Identify the components and services that will be affected. Try to minimize the impact on users by using strategies like rolling updates.
1. **Notify users**: If the maintenance will affect users, [notify them in advance](#maintenance-email-generator). Make sure to give users plenty of time to prepare for the maintenance. In general, one week's notice is recommended.
1. **Perform the maintenance**: Follow the steps outlined in the maintenance plan. If the maintenance runs over the scheduled end time, notify users of the delay.
1. **Verify the maintenance**: After the maintenance is complete, verify that all components are working as expected (including CI pipelines). If there are any issues, address them immediately. Use [observability tools](./observability) to monitor the health of the system.
1. **Notify users**: Once the maintenance is complete, update the maintenance announcement to indicate that the maintenance is complete. If there were any issues during the maintenance, provide details on what happened and how it was resolved.

## SLURM

This section outlines the maintenance procedures for the SLURM cluster.

### Cluster overview

To get a general overview of the health of the SLURM cluster, you can run:

```bash copy
sinfo --long
```

Example output:

```
Thu Apr 18 17:16:26 2024
PARTITION AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE RESERVATION NODELIST
compute*     up 1-00:00:00 1-infinite   no       NO        all      1     drained             tr-slurm1
compute*     up 1-00:00:00 1-infinite   no       NO        all      1       mixed             thor-slurm1
compute*     up 1-00:00:00 1-infinite   no       NO        all      3        idle             trpro-slurm[1-2],wato2-slurm1
```

In the output above, `tr-slurm1` is in the `drained` state, which means it is not available for running jobs.
`thor-slurm1` is in the `mix` state, which means some jobs are running on it.
All other nodes are in the `idle` state, which means there are no jobs running on them.

To get a detailed overview of nodes in the cluster, you can run:

```bash copy
scontrol show node [NODE_NAME]
```

The optional `NODE_NAME` argument can be used to restrict the output to a specific node.

Example output:

```text {11,20}
> scontrol show node tr-slurm1
NodeName=tr-slurm1 Arch=x86_64 CoresPerSocket=1
   CPUAlloc=0 CPUEfctv=58 CPUTot=60 CPULoad=0.01
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=gpu:grid_p40:1(S:0),shard:grid_p40:8K(S:0),tmpdisk:100K
   NodeAddr=tr-slurm1.ts.watonomous.ca NodeHostName=tr-slurm1 Version=23.11.4
   OS=Linux 5.15.0-100-generic #110-Ubuntu SMP Wed Feb 7 13:27:48 UTC 2024
   RealMemory=39140 AllocMem=0 FreeMem=29723 Sockets=60 Boards=1
   CoreSpecCount=2 CPUSpecList=58-59 MemSpecLimit=2048
   State=IDLE+DRAIN ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=compute
   BootTime=2024-03-17T03:32:45 SlurmdStartTime=2024-04-13T20:55:32
   LastBusyTime=2024-04-16T19:16:13 ResumeAfterTime=None
   CfgTRES=cpu=58,mem=39140M,billing=58,gres/gpu=1,gres/shard=8192,gres/tmpdisk=102400
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/a ExtSensorsWatts=0 ExtSensorsTemp=n/a
   Reason=Performing maintenance on baremetal [root@2024-04-18T17:06:20] 
```

In the output above, we can see that the reason `tr-slurm1` is in the `drained` state (a.k.a. `IDLE+DRAIN`) for reason `Performing maintenance on baremetal`.
The `Reason` field is an arbitrary user-specified string that can be set when performing actions on nodes.

### Performing maintenance on a node

#### Creating a reservation

The first step to performing maintenance on a node is to create a reservation[^slurm-reservation].
This ensures that user-submitted jobs do not get dispatched to the target nodes if they cannot be
completed before the maintenance window starts.

[^slurm-reservation]: The official documentation for reservations is at https://slurm.schedmd.com/reservations.html

```bash copy
scontrol create reservation starttime=<START_TIME> duration=<DURATION_IN_MINUTES> user=root flags=maint nodes=<NODE_NAMES>
```

The time zone for the `starttime` argument is the local time zone where the command is run.
To see the local timezone, run `timedatectl`.

Here's an example:

```bash copy
scontrol create reservation starttime=2024-04-30T21:00:00 duration=480 user=root flags=maint nodes=trpro-slurm1,trpro-slurm2
```

output:

```text
Reservation created: root_4
```

This command creates a reservation named `root_4`. The reservation starts on
April 30, 2024, at 9:00 PM local time and lasts for
8 hours (480 minutes) for the nodes `trpro-slurm1` and `trpro-slurm2`.
During this time, only the root user can run jobs on these nodes.
Jobs submitted by other users will be queued until the reservation is over.

To see existing reservations, you can run:

```bash copy
scontrol show reservation
```

To delete a resservation, you can run:

```bash copy
scontrol delete reservation <RESERVATION_NAME>
```


#### Starting maintenance

Before performing maintenance on a node, you should drain the node to ensure no jobs are running on it and no new jobs are scheduled to run on it.

```bash
scontrol update nodename="<NODE_NAME>" state=drain reason="Performing maintenance for reason X"
```

For example:

```bash copy
scontrol update nodename="tr-slurm1" state=drain reason="Performing maintenance on baremetal"
```

This will drain the node `tr-slurm1` (prevent new jobs from running on it) and set the reason to `Performing maintenance on baremetal`.
If there are no jobs running on the node, the node state becomes `drained` (a.k.a. `IDLE+DRAIN` in `scontrol`).
If there are jobs running on the node, the node state becomes `draining` (a.k.a. `MIXED+DRAIN` in `scontrol`).
In this case, SLURM will wait for the jobs to finish before transitioning the node to the `drained` state.

Example output from when a node is in the `draining` state:

```text {4,18,27}
> sinfo --long
Thu Apr 18 17:17:35 2024
PARTITION AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE RESERVATION NODELIST
compute*     up 1-00:00:00 1-infinite   no       NO        all      1    draining             tr-slurm1
compute*     up 1-00:00:00 1-infinite   no       NO        all      1       mixed             thor-slurm1
compute*     up 1-00:00:00 1-infinite   no       NO        all      3        idle             trpro-slurm[1-2],wato2-slurm1

> scontrol show node tr-slurm1
NodeName=tr-slurm1 Arch=x86_64 CoresPerSocket=1
   CPUAlloc=1 CPUEfctv=58 CPUTot=60 CPULoad=0.00
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=gpu:grid_p40:1(S:0),shard:grid_p40:8K(S:0),tmpdisk:100K
   NodeAddr=tr-slurm1.ts.watonomous.ca NodeHostName=tr-slurm1 Version=23.11.4
   OS=Linux 5.15.0-100-generic #110-Ubuntu SMP Wed Feb 7 13:27:48 UTC 2024
   RealMemory=39140 AllocMem=512 FreeMem=29688 Sockets=60 Boards=1
   CoreSpecCount=2 CPUSpecList=58-59 MemSpecLimit=2048
   State=MIXED+DRAIN ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=compute
   BootTime=2024-03-17T03:32:45 SlurmdStartTime=2024-04-13T20:55:32
   LastBusyTime=2024-04-18T17:15:30 ResumeAfterTime=None
   CfgTRES=cpu=58,mem=39140M,billing=58,gres/gpu=1,gres/shard=8192,gres/tmpdisk=102400
   AllocTRES=cpu=1,mem=512M,gres/tmpdisk=300
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/a ExtSensorsWatts=0 ExtSensorsTemp=n/a
   Reason=Performing maintenance on baremetal [root@2024-04-18T17:16:01]
```

After jobs finish running on the node, the node will transition to the `drained` state:

```text {4,18,27}
> sinfo --long
Thu Apr 18 17:22:07 2024
PARTITION AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE RESERVATION NODELIST
compute*     up 1-00:00:00 1-infinite   no       NO        all      1     drained             tr-slurm1
compute*     up 1-00:00:00 1-infinite   no       NO        all      1       mixed             thor-slurm1
compute*     up 1-00:00:00 1-infinite   no       NO        all      3        idle             trpro-slurm[1-2],wato2-slurm1

> scontrol show node tr-slurm1
NodeName=tr-slurm1 Arch=x86_64 CoresPerSocket=1
   CPUAlloc=0 CPUEfctv=58 CPUTot=60 CPULoad=0.00
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=gpu:grid_p40:1(S:0),shard:grid_p40:8K(S:0),tmpdisk:100K
   NodeAddr=tr-slurm1.ts.watonomous.ca NodeHostName=tr-slurm1 Version=23.11.4
   OS=Linux 5.15.0-100-generic #110-Ubuntu SMP Wed Feb 7 13:27:48 UTC 2024
   RealMemory=39140 AllocMem=0 FreeMem=29688 Sockets=60 Boards=1
   CoreSpecCount=2 CPUSpecList=58-59 MemSpecLimit=2048
   State=IDLE+DRAIN ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=compute
   BootTime=2024-03-17T03:32:45 SlurmdStartTime=2024-04-13T20:55:32
   LastBusyTime=2024-04-18T17:21:13 ResumeAfterTime=None
   CfgTRES=cpu=58,mem=39140M,billing=58,gres/gpu=1,gres/shard=8192,gres/tmpdisk=102400
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/a ExtSensorsWatts=0 ExtSensorsTemp=n/a
   Reason=Performing maintenance on baremetal [root@2024-04-18T17:16:01]
```

Once the node is in the `drained` state, you can perform maintenance on it.

#### Taking a node out of maintenance mode

To take a node out of maintenance mode, you can run:

```bash
scontrol update nodename="<NODE_NAME>" state=resume
```

For example:

```bash copy
scontrol update nodename="tr-slurm1" state=resume
```

This will resume the node `tr-slurm1` (allow new jobs to run on it) and clear the reason.

Also remember to delete any unexpired reservations (as outlined in [Creating a reservation](#creating-a-reservation)). 


## Maintenance Email Generator

Fill out the fields below to generate a maintenance announcement email. You can copy the generated email and send it to the [mailing list](https://groups.google.com/a/watonomous.ca/g/watcloud-compute-cluster-announcements).

import MaintenanceEmailGenerator from "@/components/maintenance-email-generator";

<MaintenanceEmailGenerator />


## Troubleshooting

### SLURM `Invalid user id` when creating a reservation

If you get `Error creating the reservation: Invalid user id` when creating a reservation, it may mean that you don't have sufficient permissions to create a reservation. You can try running the command with `sudo`. For example:

```
> sudo $(which scontrol) create reservation starttime=2025-04-27T19:00:00 duration=120 user=root flags=maint nodes=thor-slurm1
Reservation created: root_12
```

### SLURM `Requested nodes are busy` when creating a reservation

If you get `Error creating the reservation: Requested nodes are busy` when creating a reservation, it means that the nodes you are trying to reserve are already in use. E.g. there are jobs on the node with end time during the reservation window.

```
> scontrol create reservation starttime=2025-04-27T19:00:00 duration=120 user=root flags=maint nodes=thor-slurm1
Error creating the reservation: Requested nodes are busy
```

You can check which jobs are running on the node with:

```bash copy
squeue --nodelist=<NODE_NAME>
```

For example:

```bash
> squeue --nodelist=thor-slurm1
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            156143   compute slurm-sl watcloud  R       0:03      1 thor-slurm1
            156144   compute slurm-sl watcloud  R       0:02      1 thor-slurm1
            156133   compute slurm-sl watcloud  R       0:23      1 thor-slurm1
```

You can view the end time of a job with:

```bash
scontrol show job <JOB_ID>
```

For example:

```bash {10}
> scontrol show job 156143
JobId=156143 JobName=slurm-slurm-runner-large-41221285582
   UserId=watcloud-slurm-ci(1814) GroupId=watcloud-slurm-ci(1814) MCS_label=N/A
   Priority=1116 Nice=0 Account=watonomous-watcloud QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:38 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2025-04-27T05:39:27 EligibleTime=2025-04-27T05:39:27
   AccrueTime=2025-04-27T05:39:29
   StartTime=2025-04-27T05:39:29 EndTime=2025-04-27T06:09:29 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-04-27T05:39:29 Scheduler=Main
   Partition=compute AllocNode:Sid=10.1.77.47:21
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=thor-slurm1
   BatchHost=thor-slurm1
   NumNodes=1 NumCPUs=4 NumTasks=1 CPUs/Task=4 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=4,mem=8G,node=1,billing=4,gres/tmpdisk=16384
   AllocTRES=cpu=4,mem=8G,node=1,billing=4,gres/tmpdisk=16384
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=4 MinMemoryCPU=2G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/watcloud-slurm-ci/apptainer.sh
   WorkDir=/home/watcloud-slurm-ci
   StdErr=/var/log/slurm-ci/slurm-ci-156143.out
   StdIn=/dev/null
   StdOut=/var/log/slurm-ci/slurm-ci-156143.out
   TresPerNode=gres/tmpdisk:16384
   TresPerTask=cpu=4
```

If the job is long-running and you think it will finish before the maintenance starts, you can force create the reservation with the `ignore_jobs` flag. For example:

```bash
> scontrol create reservation starttime=2025-04-27T19:00:00 duration=120 user=root flags=maint,ignore_jobs nodes=thor-slurm1
Reservation created: root_12
```

Before the maintenance starts, just make sure that the job is finished before proceeding.
</file>

<file path="docs/community-docs/watcloud/observability.mdx">
# Observability

Observability is the ability to understand the internal state of a system based on its external outputs.
The term originated in control theory and has since been adopted by the software industry to describe the same concept in the context of software systems[^observability-software].

[^observability-software]: See this [Wikipedia article](https://en.wikipedia.org/wiki/Observability_(software)) for more information.

At WATcloud, we have a number of tools to help us understand the state of our systems, detect issues, and optimize processes.

## Healthchecks

Healthchecks are periodic checks that verify that a service is running as expected.
Each healthcheck typically outputs a simple `up` or `down` status.
This section describes the healthcheck infrastructure at WATcloud.

### Quick Links
- [Status Page][status-page]
- [Healthchecks.io][healthchecks-io]
- [Sentry Crons][sentry-crons]
- [Alertmanager][alertmanager]

[status-page]: https://status.watonomous.ca/
[healthchecks-io]: https://healthchecks.io/
[sentry-crons]: https://watonomous.sentry.io/crons
[alertmanager]: https://prometheus.watonomous.teleport.sh/alerts

### Status Page

The [status page][status-page] is a collection of healthchecks from various sources.
The goal of the status page is to provide a single source of truth for the status of all our services.
We regularly use the status page as a troubleshooting tool to quickly identify the source of an issue.

### Healthchecks.io

[Healthchecks.io][healthchecks-io] is a [dead man's switch (DMS)](https://en.wikipedia.org/wiki/Dead_man%27s_switch) service that accepts periodic pings from services.
When a service fails to ping the service within a specified time frame, Healthchecks.io marks the service as down.
It can be configured to send alerts to various channels. Currently, we receive alerts on Discord.

### Sentry Crons

Similar to Healthchecks.io, [Sentry Crons][sentry-crons] is a DMS service.
We receive alerts on Discord when a service fails to ping Sentry Crons within a specified time frame.

### Alertmanager

[Alertmanager][alertmanager] is a component of the Prometheus monitoring system.
It uses [metrics](#metrics) collected by Prometheus to send alerts to various channels.
Currently, we receive alerts on Discord.

## Metrics

Metrics are quantitative measurements of a system's status and performance.
This section describes the metrics infrastructure at WATcloud.

### Quick Links
- [Prometheus][prometheus]

[prometheus]: https://prometheus.watonomous.teleport.sh/

### Prometheus

[Prometheus][prometheus] is an open-source monitoring and alerting toolkit.
It collects metrics from various sources and stores them in a time-series database.
We use Prometheus to monitor the health of our systems and to set up alerts (via [Alertmanager](#alertmanager)) for potential issues.

## Logs

Logs are records of events that happen in a system.
Examples of logs include Linux system logs and Kubernetes container logs.
This section describes the logging infrastructure at WATcloud.

### Quick Links
- [Elastic Cloud][elastic-cloud]

[elastic-cloud]: https://wato-elastic-cloud-deployment.kb.us-east4.gcp.elastic-cloud.com:9243/

### Elastic Cloud

[Elastic Cloud][elastic-cloud] is a managed Elasticsearch service.
It is used to store logs from various sources, including Kubernetes clusters and Linux servers.

## Error Tracking

Error tracking is the practice of recording and monitoring errors that occur in a system.
This section describes the error tracking infrastructure at WATcloud.

### Quick Links
- [Sentry][sentry]

[sentry]: https://watonomous.sentry.io/

### Sentry

[Sentry][sentry] is an open-source error tracking tool.
It captures and aggregates errors from various sources, including web applications and backend services.
Use cases for Sentry at WATcloud include error monitoring for websites, APIs, and CI pipelines.

## Tracing

Tracing is the practice of recording the life cycle of an object.
An example of tracing is recording the different stages of a CI pipeline (e.g. start/finish times of each job stage, stages passed/failed).

Currently, WATcloud does not have a tracing system in place.
You can following along [this internal issue](https://github.com/WATonomous/infra-config/issues/1795) for updates on the status of our tracing infrastructure.
</file>

<file path="docs/community-docs/watcloud/proxmox.mdx">
import { Steps } from 'nextra/components'
import Picture from '@/components/picture'
import { DocProxmoxPrimaryGpu } from '@/build/fixtures/images'

# Proxmox

[Proxmox Virtual Environment](https://www.proxmox.com/en/proxmox-virtual-environment/overview) is a virtualization platform that we use to manage our virtual machines (VMs) at WATcloud.
Most[^most-run-proxmox] bare-metal machines use the Proxmox installation disk for OS installation.

[^most-run-proxmox]: Some exceptions are PiKVMs and possible future machines that may run other specialized OS.

This page contains guides for common tasks related to Proxmox.

## Provisioning a new Linux VM

This guide walks through the process of provisioning a new virtual machine (VM) on WATcloud.
Most changes are made in the internal [infra-config](https://github.com/watonomous/infra-config) repo.
Please familiarize yourself with the [development process](./development-manual) before proceeding.

In this guide, we will provision a new VM named `ha-microk8s1` with the following specifications:
- The VM will be used to run a [microk8s](https://microk8s.io/) node in the on-prem Kubernetes cluster.
- The VM will make use of Proxmox's [HA](https://pve.proxmox.com/wiki/High_Availability) feature to automatically migrate away from downed nodes.
- The VM will have a university-provided hostname so that it can receive traffic directly (without going through [NAT](https://en.wikipedia.org/wiki/Network_address_translation)).

<Steps>
### Create a Terraform configuration for the new VM

The following is a standard Terraform configuration for an HA VM on WATcloud.
Please review it carefully and adjust it as needed for your use case.
For other types of VMs (e.g. non-HA development VMs), please refer to the repo for examples.

Please note the following:
- This VM is using the `vmbr0` network bridge. This bridge is connected directly to the university network and is used when the VM needs to be accessed directly from the internet. The MAC address in the configuration is registered with the university. For a list of available MAC addresses for VMs, please refer to the latest version of [this configuration](https://github.com/WATonomous/infra-config/blob/ee365f90cc8fa79ffb1313a4e1c5ce025c4d4cf1/directory/hosts/host-config.yml#L2-L34).
- This VM is using `all_disks_pool` Ceph pool for root disk storage.  An HA filesystem is required for HA VMs.
- This VM is created in the Terragrunt project `proxmox/misc-service-machines`. We will run the provisioner for this project later.
- This VM is cloned from the template `ubuntu-22-04-cloudinit-v1`. This template is created using [this script](https://github.com/WATonomous/infra-config/blob/ee365f90cc8fa79ffb1313a4e1c5ce025c4d4cf1/scripts/create_vm_template_ubuntu_22_04.sh).
- This VM will be created on the node `wato-thor`. This is just for the initial deployment. After the VM is created, the Proxmox HA agent will manage the VM's placement.

```diff
diff --git a/proxmox/misc-service-machines/ha-microk8s1.tf b/proxmox/misc-service-machines/ha-microk8s1.tf
new file mode 100644
index 0000000000..8b157e1e13
--- /dev/null
+++ b/proxmox/misc-service-machines/ha-microk8s1.tf
@@ -0,0 +1,75 @@
+# A microk8s VM that runs using Proxmox High Availability (HA) features.
+# When the host node fails, the VM will be automatically migrated to another node.
+
+resource "proxmox_vm_qemu" "ha-microk8s1" {
+  force_recreate_on_change_of = "v0.0.1"
+
+  name = "ha-microk8s1"
+  desc = "(Provisioned by Terraform) This VM is used for backing up shared storages"
+  target_node = "wato-thor" # just for initial deployment
+
+  clone = "ubuntu-22-04-cloudinit-v1"
+
+  bios = "ovmf"
+  cpu = "host"
+  cores = 2
+  memory = "10240" # MB
+  scsihw = "virtio-scsi-pci"
+
+  ciuser = "watonomous"
+  ipconfig0 = "ip=dhcp"
+  ipconfig1 = "ip=dhcp"
+  qemu_os = "l26"
+
+  agent = 1
+  hastate = "started"
+  automatic_reboot = false
+
+  disk {
+    cache = "none"
+    discard = "on"
+    size = "25G"
+    storage = "all_disks_pool"
+    type = "scsi"
+  }
+
+  network {
+    bridge = "vmbr0"
+    macaddr   = "76:61:74:6f:30:38" # Preset MAC address registered with the university. Needs to be in sync with host-config
+    model = "virtio"
+  }
+
+  network {
+    bridge = "vmbr1"
+    model = "virtio"
+  }
+
+  lifecycle {
+    ignore_changes = [
+      # Ignore changes in target_node because this is an HA VM
+      target_node,
+      # We should not change the VM after cloning
+      clone,
+      full_clone,
+      # The following attributes are inherited from the template and should not be changed
+      qemu_os,
+      ipconfig0,
+      ipconfig1,
+      network,
+      ciuser,
+      agent,
+      sshkeys,
+      # These attributes are unstable after the initial creation. They change in value and don't get
+      # updated in the Terraform state. This is probably a bug in the Proxmox provider.
+      hostpci,
+      # These attributes cause VM restarts when changed. We'll manage them manually.
+      disk,
+      # These attributes show up as changes when we try to import the VM.
+      additional_wait,
+      automatic_reboot,
+      clone_wait,
+      define_connection_info,
+    ]
+    prevent_removal = true
+  }
+}
```

### Apply the Terraform configuration

In the development container, run the following commands to apply the Terraform configuration:

```bash
./proxmox/provision.sh misc-service-machines
```

Please ensure that the VM is the only change to be performed by the provisioner.
If changes to another VM is shown, someone probably forgot to commit their changes.
Please abort and consult with the team before proceeding.

Example output:
```
root@ea158e5a1675:/infra-config# ./proxmox/provision.sh misc-service-machines
Agent pid 11
Found valid cached password. Using it.
Not in dry run. Running `terragrunt apply`
proxmox_lxc.alertmanager: Refreshing state... [id=wato-derek1/lxc/102]
proxmox_vm_qemu.elastic: Refreshing state... [id=wato-delta/qemu/103]
proxmox_vm_qemu.backup: Refreshing state... [id=wato-wato1/qemu/127]
proxmox_vm_qemu.docker-registry: Refreshing state... [id=wato-wato2/qemu/101]
proxmox_vm_qemu.grafana: Refreshing state... [id=wato-wato1/qemu/118]

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # proxmox_vm_qemu.ha-microk8s1 will be created
  + resource "proxmox_vm_qemu" "ha-microk8s1" {
      + additional_wait             = 5
      + agent                       = 1
      + automatic_reboot            = false
      + balloon                     = 0
      + bios                        = "ovmf"
      + boot                        = (known after apply)
      + bootdisk                    = (known after apply)
      + ciuser                      = "watonomous"
      + clone                       = "ubuntu-22-04-cloudinit-v1"
      + clone_wait                  = 10
      + cores                       = 2
      + cpu                         = "host"
      + default_ipv4_address        = (known after apply)
      + define_connection_info      = true
      + desc                        = "(Provisioned by Terraform) This VM is used for backing up shared storages"
      + force_create                = false
      + force_recreate_on_change_of = "v0.0.1"
      + full_clone                  = true
      + guest_agent_ready_timeout   = 100
      + hastate                     = "started"
      + hotplug                     = "network,disk,usb"
      + id                          = (known after apply)
      + ipconfig0                   = "ip=dhcp"
      + ipconfig1                   = "ip=dhcp"
      + kvm                         = true
      + memory                      = 10240
      + name                        = "ha-microk8s1"
      + nameserver                  = (known after apply)
      + onboot                      = false
      + oncreate                    = true
      + preprovision                = true
      + qemu_os                     = "l26"
      + reboot_required             = (known after apply)
      + scsihw                      = "virtio-scsi-pci"
      + searchdomain                = (known after apply)
      + sockets                     = 1
      + ssh_host                    = (known after apply)
      + ssh_port                    = (known after apply)
      + tablet                      = true
      + target_node                 = "wato-thor"
      + unused_disk                 = (known after apply)
      + vcpus                       = 0
      + vlan                        = -1
      + vmid                        = (known after apply)

      + disk {
          + backup             = true
          + cache              = "none"
          + discard            = "on"
          + file               = (known after apply)
          + format             = (known after apply)
          + iops               = 0
          + iops_max           = 0
          + iops_max_length    = 0
          + iops_rd            = 0
          + iops_rd_max        = 0
          + iops_rd_max_length = 0
          + iops_wr            = 0
          + iops_wr_max        = 0
          + iops_wr_max_length = 0
          + iothread           = 0
          + mbps               = 0
          + mbps_rd            = 0
          + mbps_rd_max        = 0
          + mbps_wr            = 0
          + mbps_wr_max        = 0
          + media              = (known after apply)
          + replicate          = 0
          + size               = "15G"
          + slot               = (known after apply)
          + ssd                = 0
          + storage            = "all_disks_pool"
          + storage_type       = (known after apply)
          + type               = "scsi"
          + volume             = (known after apply)
        }

      + network {
          + bridge    = "vmbr0"
          + firewall  = false
          + link_down = false
          + macaddr   = "76:61:74:6f:30:35"
          + model     = "virtio"
          + queues    = (known after apply)
          + rate      = (known after apply)
          + tag       = -1
        }
      + network {
          + bridge    = "vmbr1"
          + firewall  = false
          + link_down = false
          + macaddr   = (known after apply)
          + model     = "virtio"
          + queues    = (known after apply)
          + rate      = (known after apply)
          + tag       = -1
        }
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: 
```

If everything looks good, type `yes` and press Enter to apply the changes.
If you waited too long (more than around 30 seconds), you may get a `401 authentication failure` error.
This is because the [TOTP](https://en.wikipedia.org/wiki/Time-based_one-time_password) code has expired.
Simply rerun the command to continue.

After confirming, you should see the following output:

```
proxmox_vm_qemu.ha-microk8s1: Creating...
proxmox_vm_qemu.ha-microk8s1: Still creating... [10s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [20s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [30s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [40s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [50s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [1m0s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [1m10s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [1m20s elapsed]
proxmox_vm_qemu.ha-microk8s1: Still creating... [1m30s elapsed]
proxmox_vm_qemu.ha-microk8s1: Creation complete after 1m36s [id=wato-thor/qemu/105]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
Wrote outputs to ./outputs/proxmox/misc-service-machines/outputs.yaml and ./outputs/proxmox/misc-service-machines/outputs.secret.yaml
Wrote Terraform state to ./outputs/proxmox/misc-service-machines/state.encrypted.dynamic.yaml
```

### Manually populate the SSH public key for the provisioner

The newer `sshd` servers appear to disable password authentication by default.
To allow the provisioner to SSH into the VM, we need to manually populate the SSH public key for the provisioner.
To do this, find the SSH public key and add it to the VM's authorized keys file.
At the time of writing, the SSH public key is [stored in the `infra-config` repo](https://github.com/WATonomous/infra-config/blob/ee365f90cc8fa79ffb1313a4e1c5ce025c4d4cf1/secrets/secrets.yml#L414).

To access the VM to place the SSH key, we can use the serial terminal.
SSH into the host machine (`wato-thor` in this case, credentials are in 1Password WATcloud vault under `Proxmox`) and run the following command.
Once the serial terminal is open, you can get the login prompt by pressing <key>Enter</key>.

```bash
root@wato-thor:~# qm terminal 105
starting serial terminal on interface serial0 (press Ctrl+O to exit)

ha-microk8s1 login:
```

The username and password for the VM are stored in 1Password under the `WATonomous VM User` entry.
You may need to use one of the old passwords listed, because the VM template may be created before the latest password change.

During the initial provision, [cloud-init](https://github.com/canonical/cloud-init) will automatically provision users, networking, and other configurations.
It may be a few minutes before the default user is created.
The cloud-init logs will appear in the serial console.
After you see the following message, the user should be created:

```
ci-info: no authorized SSH keys fingerprints found for user watonomous.
<14>Aug 17 00:24:12 cloud-init: #############################################################
<14>Aug 17 00:24:12 cloud-init: -----BEGIN SSH HOST KEY FINGERPRINTS-----
<14>Aug 17 00:24:12 cloud-init: 1024 SHA256:DZJKZ+0GN05s0JPtepgd1SnD4WN575LWqNw25v6oicY root@ha-microk8s1 (DSA)
<14>Aug 17 00:24:12 cloud-init: 256 SHA256:bsafc+Q5HAkHbIdKmkEs153//cmo4dGr0qnRq8PnMrg root@ha-microk8s1 (ECDSA)
<14>Aug 17 00:24:12 cloud-init: 256 SHA256:M1HFT3xyGjgbBHWJOR7EvV6agzbah7YFaEd8eXrnDcA root@ha-microk8s1 (ED25519)
<14>Aug 17 00:24:12 cloud-init: 3072 SHA256:WGcM+4EfykW7SnaclZ5P/vSzkDQjKRfWpwHE9grrPFY root@ha-microk8s1 (RSA)
<14>Aug 17 00:24:12 cloud-init: -----END SSH HOST KEY FINGERPRINTS-----
<14>Aug 17 00:24:12 cloud-init: #############################################################
-----BEGIN SSH HOST KEY KEYS-----
ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFqZEkE1h7WcCyIaEJxdkhKjgKWGFzfpQIPzJMzuY1pW56hBsRaKLf5C6pZcCsmCx7wn4MsrX4Ki4aNR9/B2qVY= root@ha-microk8s1
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINYvjkcFIMoq9BNNXSSGiTs3/fVHroFmB1tZh+1XYVj0 root@ha-microk8s1
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC4EJfO2EOH6SFkVWR64OQSHY3cY7UCFKvOYiq/2XyT6GeevA3dbGf3JiAS3XEZh5QoAW/tZ9d7SNKAN3cagbsL7DYVbOgJh37L3hJ8av/B9BUiDWFh3Xd1o4eBsl3SA1RV6ftxS/aKlt78D/JyWsX/F9QKSwU1z7hXd+lpb5OUU0GNA1q+0wdVK14hOUiem06iBmabIn6KYLexVsMkKt5H/EYVmDkgMqF/rnWRhfUiabMwqDrp2QNOTCPCgEr65PGAx7reWkkZw236RoQjp3puR0RgdJTMriLLabqxMxstC7zqWQaigr6q/BWtzmKGkZUrK2859SsrbH2dWGbLygzDGksW41mEL27jaecl7ocihsB5U3jX68LowVcQD+UwzIBFMlprZEpXzw1Od2ouXkRdgTQGsxV2c63vZdlNP7wecH/TdUtAkmvvjyvXH15EEmwSYL7LL/b4ciC9UFeBP8cjS2JW1af2tjfX3kIhdyJrbpnJQP4MqIHz8KfFYBG8488= root@ha-microk8s1
-----END SSH HOST KEY KEYS-----
[  342.249334] cloud-init[1352]: Cloud-init v. 23.3.1-0ubuntu1~22.04.1 finished at Sat, 17 Aug 2024 00:24:12 +0000. Datasource DataSourceNoCloud [seed=/dev/sr0][dsmode=net].  Up 342.24 seconds
```

After entering the username and password, you should be able to access the VM:

```
ha-microk8s1 login: watonomous
Password:
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-87-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Sat Aug 17 00:27:55 UTC 2024

  System load:           0.0
  Usage of /:            26.9% of 14.36GB
  Memory usage:          2%
  Swap usage:            0%
  Processes:             115
  Users logged in:       0
  IPv4 address for eth0: 129.97.29.205
  IPv6 address for eth0: 2620:101:f000:8202:7461:74ff:fe6f:3035
  IPv4 address for eth1: 10.0.50.225

Expanded Security Maintenance for Applications is not enabled.

172 updates can be applied immediately.
116 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status



The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

watonomous@ha-microk8s1:~$
```

To populate the SSH public key, simply add it to the `~/.ssh/authorized_keys` file:

```bash
# Note that the following key is subject to change. Please make sure you have the latest key.
echo "ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBA/JWeGTFa+jlntmVWczbqeVIKndgaMj1Pyh2u3juzyEnllONTrWMZUI1e8RaEPS4rcXPxb9yDAhsBxZSCY5LWI= wato_ansible_key" >> ~/.ssh/authorized_keys
```

### Restart the VM to apply network changes

After cloud-init has finished, the VM will have the correct hostname.
Restart the VM to trigger a DHCP reset[^dhcp-restart].
This will allow the cluster switch to receive the new hostname and assign a temporary `.watocluster.local` domain for initial provisoining.

[^dhcp-restart]: This may also be done some other way (e.g. using `dhclient`). However, restarting the VM has always worked for us and is a simple solution without digging into the details.

After the VM has restarted, you should be able to look it up using the cluster DNS server (`10.0.50.254`):

```
root@wato-thor:~# nslookup ha-microk8s1.watocluster.local 10.0.50.254
Server:		10.0.50.254
Address:	10.0.50.254#53

Non-authoritative answer:
Name:	ha-microk8s1.watocluster.local
Address: 10.0.50.225
```

### Create the Ansible configuration for the new VM

Now, it's time to update the host registry with the new VM's configuration.
This configuration can be derived from other VM configurations of the same type.
Because we configured the VM to use a MAC address for the university network, we update the corresponding entry in the configuration.

Note that we left some parts of the configuration commented out. Specifically, the parts related to microk8s.
This is because there is a circular reference--some parts of the repo (namely DNS management) depends on the microk8s configuration,
and provisioning VMs depends on the DNS configuration. We will add back the microk8s configuration after the base configuration is applied.

Also note that `ansible_hosts` contains the `.watocluster.local` domain that is automatically assigned by the cluster switch.

```diff
diff --git a/directory/hosts/host-config.yml b/directory/hosts/host-config.yml
index 2b1967594d..7d02d1eaf9 100644
--- a/directory/hosts/host-config.yml
+++ b/directory/hosts/host-config.yml
@@ -26,7 +26,7 @@ _anchors:
     - name: &derek2_microk8s1_ext_hostname_cname_target cph-wato-vm7.uwaterloo.ca
       mac_address: 76:61:74:6f:30:37
       notes: Ports 80 and 443 are exposed externally
-    - name: cph-wato-vm8.uwaterloo.ca # manually parked at prometheus lxc container
+    - name: &ha_microk8s1_ext_hostname_cname_target cph-wato-vm8.uwaterloo.ca
       mac_address: 76:61:74:6f:30:38
       notes: Ports 80 and 443 are exposed externally
     - name: &tr_ubuntu1_ext_hostname_cname_target cph-wato-vm9.uwaterloo.ca
@@ -65,6 +65,7 @@ _anchors:
     - &wato2-microk8s1-cluster-ip 10.0.50.152
     - &tr-microk8s1-cluster-ip 10.0.50.153
     - &derek2-microk8s1-cluster-ip 10.0.50.154
+    - &ha-microk8s1-cluster-ip 10.0.50.155
     # Slurm VMs
     - &tr-slurm1-cluster-ip 10.0.50.180
     - &wato2-slurm1-cluster-ip 10.0.50.181
@@ -1566,6 +1567,63 @@ hosts:
       - name: swap_nodes
       - name: filebeat_nodes
         elastic_host: elastic.cluster.watonomous.ca
+  - name: ha-microk8s1
+    ansible_hosts:
+      - ha-microk8s1.watocluster.local
+    ansible_user: watonomous
+    networks:
+      - <<: *ens18-university-network
+        interface: eth0
+      - dns_records:
+          - name: &ha_microk8s1_ext_hostname ha-microk8s1.ext.watonomous.ca
+            type: CNAME
+            # Need to be consistent with the MAC configuration (configured in the Proxmox provisioner)
+            value: *ha_microk8s1_ext_hostname_cname_target
+        managed_by_ansible: false
+      - comments: --- Cluster Network ---
+        hostnames: [ha-microk8s1.watocluster.local]
+        interface: eth1
+        ip_address: *ha-microk8s1-cluster-ip
+        netmask_length: 24
+        dns_records:
+          - name: ha-microk8s1.cluster.watonomous.ca
+            type: A
+            value: *ha-microk8s1-cluster-ip
+        dhcp4: false
+        # TODO: Now that we have both netplan and /etc/network/interfaces, set managed_by_ansible
+        # to false by default in the schema and in the ansible config
+        managed_by_ansible: false
+        managed_by_ansible_netplan: true
+    groups:
+      - name: service_nodes
+      - name: user_nodes
+        include_users:
+          - watonomous
+          # to guard against programs adding users in the managed uid range
+          - user3000
+        include_groups:
+          - user3000
+        user_ssh_key_base_dir: /home/%u/.ssh
+        # use default permissions for the user's .ssh directory
+        use_strict_ssh_key_dir_permissions: false
+      - name: ssh_hardened_nodes
+      - name: prometheus_node_exporter_nodes
+        prometheus_target: ha-microk8s1.cluster.watonomous.ca
+      - name: rsyslog_client_nodes
+      - name: monitored_nodes
+        checks:
+          - <<: *ping-cluster-new
+      # - name: microk8s_nodes
+      #   microk8s_ingress_ext_addrs:
+      #     - *ha_microk8s1_ext_hostname 
+      #   microk8s_ingress_cluster_addrs:
+      #     - *ha-microk8s1-cluster-ip
+      # - name: microk8s_ha_nodes
+      #   microk8s_node_ip: *ha-microk8s1-cluster-ip
+      - name: swap_nodes
+      - name: filebeat_nodes
+        elastic_host: elastic.cluster.watonomous.ca
   - name: tr-slurm1
     ansible_hosts:
       - tr-slurm1.cluster.watonomous.ca
```

### Provision DNS records

Run the following command to provision the DNS records:

```bash
./cloudflare/provision.sh
```

This command will ask for confirmation before making changes. Please review the changes carefully before confirming. The changes should all be related to the new VM.

### Make sure that the VM is accessible via Ansible

After the configuration is updated, we should be able to access the VM via Ansible. Run the following command to confirm:

```
root@ea158e5a1675:/infra-config# ./ansible/run.sh ansible ha-microk8s1 -m shell -a "whoami"
Agent pid 11
Found valid cached password. Using it.
Identity added: (stdin) (wato_ansible_key)
ha-microk8s1 | CHANGED | rc=0 >>
watonomous
```

### Choose the Ansible playbook to Run

Now that the VM is accessible via Ansible, we can run the appropriate playbook to configure the VM.
At the time of writing, a few playbooks don't support running against single VMs (they must be run against the entire cluster).
We will comment them out for the initial provision.

```diff
diff --git a/ansible/playbooks/all.yml b/ansible/playbooks/all.yml
index f50dba708e..caeacb5682 100644
--- a/ansible/playbooks/all.yml
+++ b/ansible/playbooks/all.yml
@@ -23,5 +23,5 @@
 - import_playbook: ipmi.yml
 - import_playbook: swap.yml
 - import_playbook: slurmd.yml
-- import_playbook: microk8s.yml # FIXME: requires the master node to be included in the hosts list, so we can't run each host isolated using --hosts.
-- import_playbook: prometheus.yml # FIXME: this playbook requires running Ansible with all prometheus targets in the inventory. Generate the inventory before running Ansible so that Prometheus can be provisioned by itself.
+# - import_playbook: microk8s.yml # FIXME: requires the master node to be included in the hosts list, so we can't run each host isolated using --hosts.
+# - import_playbook: prometheus.yml # FIXME: this playbook requires running Ansible with all prometheus targets in the inventory. Generate the inventory before running Ansible so that Prometheus can be provisioned by itself.
```

### Run the Ansible playbook

Now, we can run the Ansible playbook to configure the VM.

```bash
./ansible/provision.sh --hosts ha-microk8s1 all
```

This command will ask for confirmation before making changes, and will take some time to complete.
The VM will automatically restart a few times during the process.

In the middle of the process, Ansible may get stuck at the following task:

```
RUNNING HANDLER [common : Apply netplan] ******************************************************************************************************************************************************************************************************************************************
```

This is because the VM's networking configuration changed and is no longer accessible via `*.watocluster.local`.
When this happens, you can interrupt the process (`Ctrl+C`) and replace the `ansible_hosts` entry in the configuration with the new domain provisioined earlier.

```diff
diff --git a/directory/hosts/host-config.yml b/directory/hosts/host-config.yml
index be503f2e43..48fd1595b8 100644
--- a/directory/hosts/host-config.yml
+++ b/directory/hosts/host-config.yml
@@ -1569,7 +1569,7 @@ hosts:
         elastic_host: elastic.cluster.watonomous.ca
   - name: ha-microk8s1
     ansible_hosts:
-      - ha-microk8s1.watocluster.local
+      - ha-microk8s1.cluster.watonomous.ca
     ansible_user: watonomous
     networks:
       - <<: *ens18-university-network
```

After making the change, rerun the Ansible playbook:

```bash
./ansible/provision.sh --hosts ha-microk8s1 all
```

The playbook should complete successfully:

```
PLAY RECAP ************************************************************************************************************************************************************************************************************************************************************************
ha-microk8s1               : ok=96   changed=30   unreachable=0    failed=0    skipped=40   rescued=0    ignored=0   
```

### Add back commented-out configuration and run the remaining Ansible playbooks

Now that the initial provisioning is complete, we can add back the commented-out configuration for the microk8s VMs.

```diff
diff --git a/directory/hosts/host-config.yml b/directory/hosts/host-config.yml
index 48fd1595b8..4af24e27c9 100644
--- a/directory/hosts/host-config.yml
+++ b/directory/hosts/host-config.yml
@@ -1613,13 +1613,13 @@ hosts:
       - name: monitored_nodes
         checks:
           - <<: *ping-cluster-new
-      # - name: microk8s_nodes
-      #   microk8s_ingress_ext_addrs:
-      #     - *ha_microk8s1_ext_hostname 
-      #   microk8s_ingress_cluster_addrs:
-      #     - *ha-microk8s1-cluster-ip
-      # - name: microk8s_ha_nodes
-      #   microk8s_node_ip: *ha-microk8s1-cluster-ip
+      - name: microk8s_nodes
+        microk8s_ingress_ext_addrs:
+          - *ha_microk8s1_ext_hostname
+        microk8s_ingress_cluster_addrs:
+          - *ha-microk8s1-cluster-ip
+      - name: microk8s_ha_nodes
+        microk8s_node_ip: *ha-microk8s1-cluster-ip
       - name: swap_nodes
       - name: filebeat_nodes
         elastic_host: elastic.cluster.watonomous.ca
```

Now, we can run the remaining Ansible playbooks.
This time, we run them against the entire cluster.

```bash
./ansible/provision.sh microk8s
./ansible/provision.sh prometheus
```

### Provision DNS records again

This step is machine-specific. In this example, we added back the microk8s configuration, which some services depend on.
This requires provisioning the DNS records with the latest configuration.

```bash
./cloudflare/provision.sh
```

### Open a PR and review changes

If you haven't already, please open a pull request (PR) and review the changes with the team.

</Steps>


## GPU Passthrough

*This note is derived from https://github.com/WATonomous/infra-notes/blob/7786dfe4cd2af74e76530285e1141d06f5ab2df2/gpu-passthrough.md*

Passing through GPUs to VMs is a complex process that requires modifying many parts of the host system.
Fortunately, the official Proxmox documentation contains a guide on how to do this: [PCI Passthrough](https://pve.proxmox.com/wiki/PCI_Passthrough).
This guide should be sufficient, and other resources (such as various forums) may provide outdated information or incorrect steps.

### Quirks

- When passing through the GPU, there might need to be some fiddling around with whether or not to enable "Primary GPU": <Picture alt="Primary GPU Setting" image={DocProxmoxPrimaryGpu} />
  - "Primary GPU" appears to be required when using GTX 1080 on Ubuntu (otherwise `nvidia-smi` throws an error) or Windows VMs (otherwise we get Error 43. Though with primary GPU enabled, we can't access the display from the Proxmox console anymore). It doesn't appear to be required when passing through an RTX 3090 to a Windows VM.
- When desktop GUI is installed and a display is plugged in, GPU passthrough will stop working. It will error out the first time the VM is started, then it will hang the second time the VM is started. Brief discussion [here](https://discord.com/channels/478659303167885314/571386709233893406/952007213742977044). The solution is to uninstall the desktop GUI or not plug in a display.

The following are some machine-specific quirks that we have encountered.

#### trpro (ASUS WRX80 Motherboard)

When a GPU is placed in the PCIe slot right beside the RAM, passthrough won't work. The error code is something about invalid VBIOS and then an error message about being stuck in power state D3. When we pass the VM a `romfile` manually according to the instructions in the Proxmox PCI passthrough doc the invalid VBIOS error goes away but we are still stuck in power state D3. ([Source](https://discord.com/channels/478659303167885314/580890419379044362/942267415071457311))

### PCI bandwidth and GPUs

PCIe gen4 appears to require a stronger signal quality than gen3.
When using PCIe risers at gen4 speeds, we encountered PCIe errors.
See [this thread](https://discord.com/channels/478659303167885314/1163159162432262264)
for more info on this.

We may be able to resolve this by replacing our current passive risers with retimers.
See [this discussion](https://github.com/WATonomous/infra-config/discussions/1938) for more information on this.
However, retimers are much more expensive than passive risers.

The current workaround is to set the link speed to gen3 or below.
This reduces the bandwidth available to the GPUs, but for a lot of workloads,
this doesn't substantially decrease the performance.
</file>

<file path="docs/community-docs/watcloud/user-requests.mdx">
# User Requests

Sometimes, users may request access to additional software packages, more disk quota, or longer SLURM job time limits.
This document outlines the process of handling such requests.

Prior to performing changes for the request, WATcloud admins should understand the request fully and consider the implications of the change.
The following questions are examples of what the admins should consider:

**Does the request make use of cluster resources in a fair and efficient manner?**

For example, if a user requests more disk quota, the WATcloud admin should consider whether the type of data
the user is storing is appropriate for the storage location.
For instance, long-term storage of large datasets on SSD-backed storage may not be the most efficient use of resources.

**Can the request be fulfilled without the help of WATcloud admins?**

Sometimes, the requested work can be done by the users themselves.
The WATcloud admin should point the user to the relevant documentation or resources, and improve the documentation if necessary.
For example, if a user requests a software package that can be installed in user-space (e.g. Conda),
the WATcloud admin should point the user to some options (e.g. [miniconda](https://docs.anaconda.com/miniconda/)).

In subsequent sections, we will discuss specific user requests and how to handle them.

## SLURM Job Time Limit Extension

Users may request an extension to the time limit of their SLURM[^slurm-docs] jobs.
The time limits are in place to ensure that the cluster remains maintainable.
For example, if the maximum time limit jobs is 7 days, then in the worst case scenario,
we need to wait 7 days for a node to be drained before performing maintenance[^slurm-maintenance] on it.

If a user requests an extension to the time limit and the request does not conflict with upcoming maintenance,
then the WATcloud admin can extend the time limit as follows:

```bash copy
scontrol update job=<job_id> TimeLimit=<new_time_limit>
```

[^slurm-docs]: See the [SLURM documentation](/docs/compute-cluster/slurm) for more information on the use of SLURM in the WATcloud cluster.
[^slurm-maintenance]: See the [Maintenance Manual](./maintenance-manual#slurm) for information on performing maintenance on SLURM nodes.

## Software Package Installation

Users may request the installation of software packages that are not already available on the cluster.
If the software package is required to be installed system-wide and the demand for the software outweighs
the cost (in terms of storage space and maintenance complexity) of installing the software,
then the WATcloud admin can add the software package to the cluster Ansible configuration[^ansible-config].

[^ansible-config]: At the time of writing, the Ansible configuration can be found [here](https://github.com/WATonomous/infra-config/blob/7d59786e61ce779f20e7cdb8c29b4864fe1b6c31/ansible/roles/ubuntu_dev_vm/tasks/main.yml#L94-L144).
    Note that removing software packages is a separate step [here](https://github.com/WATonomous/infra-config/blob/7d59786e61ce779f20e7cdb8c29b4864fe1b6c31/ansible/roles/ubuntu_dev_vm/tasks/main.yml#L27-L65).
</file>

<file path="docs/community-docs/_meta.json">
{
    "watcloud": "WATcloud"
}
</file>

<file path="docs/community-docs/watcloud.mdx">
# WATcloud Team Documentation

## Overview

As a WATcloud member, you have access to a variety of resources to help you get started and be successful.

First, you should get familiar with all of the [services](/docs/services) that WATcloud provides.
Using these services regularly (e.g. run your homework assignments and personal projects on the [compute cluster](/docs/compute-cluster))
will help you understand them from a user's perspective[^dogfooding].

[^dogfooding]: There's a term for this: [dogfooding](https://en.wikipedia.org/wiki/Eating_your_own_dog_food).

Next, you should understand the [WATcloud Guidelines](./watcloud/guidelines).
These guidelines are a set of best practices that help produce high-quality work and facilitate collaboration.

Finally, you should obtain access to various tools that the team uses.
You can do this through the [onboarding-form](/docs/utilities/onboarding-form).
Here's a list of the most important ones:
- Discord: Our primary communication platform. Select the `WATcloud` role, and `Incoming Member` if you are new, or `Core Member` after you've polished and deployed your first WATcloud project.
- GitHub: Our primary code repository. Select `WATonomous Team` to get access to internal repos.
- Compute Cluster: Our primary compute resource.
- WATcloud Internal Tools: A collection of internal tools (e.g. Prometheus) that you may find useful.
- Sentry: Our error tracking tool. Select `WATcloud Team` to get access to WATcloud projects.

As you get more involved in the team, you may need to access additional tools or upgrade your access level.
A team lead will help you with this.

## Meetings

import websiteConfig from '@/build/fixtures/website-config.json'
import { Link } from 'nextra-theme-docs'

WATcloud hosts weekly meetings every **Wednesday at 8:00 PM Eastern Time**. These meetings take place on <Link href={`https://discord.gg/${websiteConfig.discord_invite_code}`}>Discord</Link> in the `General 1` voice channel. Sometimes, we may also have meetings in-person in the server room (`CPH-3667`) to perform hardware maintenance.

Each meeting is an opportunity to review ongoing projects, plan upcoming work, and discuss technical or administrative topics.

WATcloud members are expected to attend the meetings, and anyone else is welcome to join as well.

## Index

import PageIndex from '@/components/page-index'

<PageIndex pageRoot="/docs/community-docs/watcloud" />
</file>

<file path="docs/compute-cluster/_meta.json">
{
    "overview": {
        "display": "hidden"
    },
    "getting-access": "Getting Access",
    "machine-usage-guide": "Machine Usage Guide",
    "ssh": "SSH",
    "slurm": "SLURM",
    "firewall": "Firewall",
    "quotas": "Quotas",
    "support-resources": "Support Resources"
}
</file>

<file path="docs/compute-cluster/firewall.mdx">
import { Callout } from 'nextra/components'

# Firewall

<Callout type="warning">
On 2024-12-18, the university requested that we move bastion to behind the firewall due to general security concerns.
This means that users without access to the UWaterloo campus or VPN will no longer be able to access the cluster.
We are working with the university to find a solution to this problem.
In the meantime, materials regarding accessing the bastion host from off-campus are invalid.
</Callout>

The WATcloud compute cluster is housed at the University of Waterloo. All machines in the cluster are behind the University's firewall.
In order to connect to the cluster, you must be on the campus network, connected to the University's [VPN][uw-vpn], or use a
[Bastion](#bastion) as a jump host.

[uw-vpn]: https://uwaterloo.atlassian.net/wiki/spaces/ISTKB/pages/262012980/Virtual+Private+Network+VPN

## Bastion

A Bastion (or "Bastion Host") is a machine that is exposed to the internet and is used as a gateway to access other machines that are
behind a firewall[^bastion]. At WATcloud, we host a Bastion that you can use to connect to the cluster.

[^bastion]: See the [Wikipedia page](https://en.wikipedia.org/wiki/Bastion_host) for more information.

<span className="dark:hidden">
```mermaid
flowchart
	subgraph Internet
		user <-.->|"firewall rule exception"| bastion(("bastion"))
		subgraph firewall["UWaterloo Firewall"]
			bastion <--> wato1
			bastion <--> wato2
			bastion <--> wato3
			bastion <--> dotdotdot["..."]
		end
	end

style Internet fill:transparent,stroke:#000,stroke-width:2px
style firewall fill:transparent,stroke:#F00,stroke-width:2px,color:#F00
style bastion fill:transparent,stroke:#000
style wato1 fill:transparent,stroke:#000
style wato2 fill:transparent,stroke:#000
style wato3 fill:transparent,stroke:#000
style dotdotdot fill:transparent,stroke:#000
style user fill:transparent,stroke:#000
linkStyle 0 stroke:lightgreen,stroke-width:5px
```
</span>
<span className="hidden dark:block">
```mermaid
flowchart
	subgraph Internet
		user <-.->|"firewall rule exception"| bastion(("bastion"))
		subgraph firewall["UWaterloo Firewall"]
			bastion <--> wato1
			bastion <--> wato2
			bastion <--> wato3
			bastion <--> dotdotdot["..."]
		end
	end

style Internet fill:transparent,stroke:#FFF,stroke-width:2px
style firewall fill:transparent,stroke:#F00,stroke-width:2px,color:#F00
style bastion fill:transparent,stroke:#FFF
style wato1 fill:transparent,stroke:#FFF
style wato2 fill:transparent,stroke:#FFF
style wato3 fill:transparent,stroke:#FFF
style dotdotdot fill:transparent,stroke:#FFF
style user fill:transparent,stroke:#FFF
linkStyle 0 stroke:lightgreen,stroke-width:5px
```
</span>

## VPN

A VPN (or "Virtual Private Network") is a service that allows you to connect to a private network over the internet.
The University of Waterloo provides a [VPN][uw-vpn] service that allows you to connect to the campus network from anywhere
in the world. Once you are on the UWaterloo VPN, you can connect to the WATcloud compute cluster as if you were on campus.

[uw-vpn]: https://uwaterloo.atlassian.net/wiki/spaces/ISTKB/pages/262012980/Virtual+Private+Network+VPN
</file>

<file path="docs/compute-cluster/getting-access.mdx">
---
title: "Getting Access"
---

# Getting Access to the Compute Cluster

import { Steps } from 'nextra/components'
 
<Steps>
### Determine your WATcloud contact

Your [WATcloud contact](/docs/services#watcloud-contact) will be your primary point of contact for onboarding and support.

### Fill out the onboarding form
 
Please fill out the [onboarding form](/docs/utilities/onboarding-form) and make sure to enable the "Compute Cluster" option.
 
### Get approval from your WATcloud contact

Reach out to your WATcloud contact and ask them to approve your request.

### Wait for your access to be provisioned

After your request is approved, it usually takes about 15 minutes for your access to be provisioned. Your WATcloud contact has visibility
into the provisioning pipeline and can work with the WATcloud team to resolve any technical issues that may arise.

Once your access is provisioned, you will receive a welcome email.

In the mean time, please familiarize yourself with the [Machine Usage Guide](./machine-usage-guide).
 
</Steps>
</file>

<file path="docs/compute-cluster/machine-usage-guide.mdx">
# Machine Usage Guide

This document provides an overview of the machines in the WATcloud compute cluster, including their hardware, networking, operating system, services, and software.
It also includes guidelines for using the machines, troubleshooting instructions for common issues, and information about maintenance and outages.

## Types of Machines

There are two main types of machines in the cluster: [general-use machines](/machines#general-use-machines) and [SLURM compute nodes](/machines#slurm-compute-nodes).
We will refer to them both as "development machines" in this document.

### General-Use Machines

General-use machines are meant for interactive use and are shared among all users in the cluster.
Additionally, general-use machines marked as SLURM login nodes (`SL`) in the [machine list](/machines#general-use-machines)
can be used to submit jobs to the [SLURM cluster](./slurm).

Instructions for accessing our general-use machines can be found in our [SSH documentation](./ssh).

### SLURM Compute Nodes

Simple Linux Utility for Resource Management (SLURM) is an open-source job scheduler that allocates resources to jobs on a cluster of computers.
It is widely used in HPC[^hpc] environments to provide a fair and efficient way to run jobs on a shared cluster.

Instructions for accessing our SLURM cluster can be found in our [SLURM documentation](./slurm).

[^hpc]: [High-performance computing](https://en.wikipedia.org/wiki/High-performance_computing) (HPC) is the use of supercomputers and parallel processing
  techniques to solve complex computational problems. Examples of HPC clusters include [Cedar](https://docs.computecanada.ca/wiki/Cedar) and
  [Graham](https://docs.computecanada.ca/wiki/Graham).

## Hardware

Most machines in the cluster come with standard workstation hardware that include CPU, RAM, GPU, and storage[^machine-specs]. In special
cases, you can request to have specialized hardware such as FPGAs installed in the machines.

[^machine-specs]: Machine specs can be found [here](/machines).

## Networking

All machines in the cluster are connected to both the university network (using 10Gbps or 1Gbps Ethernet)
and a cluster network (using 40Gbps or 10Gbps Ethernet). The IP address range for the university network is
`129.97.0.0/16`[^uwaterloo-ip-range] and the IP address range for the cluster network is `10.0.50.0/24`.

[^uwaterloo-ip-range]: The IP range for the university network can be found [here](https://uwaterloo.ca/information-systems-technology/about/organizational-structure/technology-integrated-services-tis/network-services-resources/ip-requests-and-registrations).

## Operating System

All development machines are virtual machines (VMs)[^hypervisor].
This setup allows us to easily manage machines remotely and reduce the complexity of the bare-metal OSes.

[^hypervisor]: We use [Proxmox](https://www.proxmox.com/en/) as our hypervisor.

## Services

### `/home` Directory

We run an SSD-backed Ceph[^ceph] cluster to provide distributed storage for machines in the cluster.
All development machines share a common `/home` directory that is backed by the Ceph cluster.

Due to the relatively expensive cost of SSDs and observations that large file transfers can slow down the filesystem for all users,
the home directory should only be used for storing small files.
If you need to store large files (e.g. datasets, videos, ML model checkpoints), please use one of the other storage options below.

[^ceph]: [Ceph](https://ceph.io/) is a distributed storage system that provides high performance and reliability.

### `/mnt/wato-drive*` Directory

We have a few HDD-backed NFS[^nfs] servers that provide large storage for machines in the cluster.
These NASes are mounted on all development machines at the `/mnt/wato-drive*` directories.
You can use these mounts to store large files such as datasets and ML model checkpoints.

[^nfs]: [NFS](https://en.wikipedia.org/wiki/Network_File_System) stands for "Network File System" and is used to share files over a network.

### `/mnt/scratch` Directory

Every general-use machine has an SSD-backed local storage pool that is mounted at the `/mnt/scratch` directory.
These storage pools are meant for temporary storage for jobs that require fast and reliable filesystem access,
such as storing training data and model checkpoints for ML workloads.

The space on `/mnt/scratch` is limited and shared between all users.
Please make sure to clean up your files frequently (after every job).
To promote good hygiene, there is an aggressive soft quota on the `/mnt/scratch` directory.
Please refer to the [Quotas](./quotas) page for more information.

Scratch space is available on SLURM compute nodes as well.
They are mounted at `/tmp` and can be requested using the [`tmpdisk` resource](./slurm#grestmpdisk).

### Docker

Every development machine has Docker Rootless[^docker-rootless] installed.
On general-use machines, the Docker daemon is automatically started[^docker-systemd] when you log in.
On SLURM compute nodes, the Docker daemon needs to be [started manually](./slurm#using-docker).

On general-use machines, the storage location for Docker is set to `/var/lib/cluster/users/$UID/docker`, where `$UID` is your user ID.
`/var/lib/cluster` is an SSD-backed storage pool, and there is a per-user storage quota to ensure that everyone has
enough space to run their workloads. Please refer to the [Quotas](./quotas) page for more information.

[^docker-rootless]: [Docker](https://www.docker.com/) is a platform for neatly packaging software, both for development and deployment.
  [Docker Rootless](https://docs.docker.com/engine/security/rootless/) is a way to run Docker without root privileges.

[^docker-systemd]: The Docker daemon is started using a systemd user service.

### S3-compatible Object storage

We have an S3-compatible object storage that runs on the Ceph cluster. If you require this functionality, please contact a WATcloud
admin to get access.

### Kubernetes

We run a Kubernetes cluster using [microk8s](https://microk8s.io/). This is mostly for running internal WATcloud services. However,
if you require this functionality, please contact a WATcloud admin to get access.

### GitHub Actions Runners

We run a GitHub Runner farm on Kubernetes using [actions-runner-controller](https://github.com/actions/actions-runner-controller).
Currently, it's enabled for the WATonomous organization. If you require this functionality, please reach out to a WATcloud admin to get access.

## Software

We try to keep the machines lean and generally refrain from installing software that make sense for rootless installation or running in
containerized environments.

Examples of software that we install:
- Docker (rootless)
- NVIDIA Container Toolkit
- zsh
- various CLI tools (e.g. `vifm`, `iperf`, `moreutils`, `jq`, `ncdu`)

Examples of software that we do not install:
- conda (use [miniconda](https://docs.conda.io/en/latest/miniconda.html) instead)
- ROS (use [Docker](https://hub.docker.com/_/ros) instead)
- CUDA (use [Docker](https://hub.docker.com/r/nvidia/cuda) instead. Or use [CVMFS](./slurm#cvmfs) on the SLURM compute nodes.)

If there is a piece of software that you think should be installed on the machines, please reach out to a WATcloud team member.

## Maintenance and Outages

We try to keep machines in the cluster up and running at all times. However, we need to perform regular maintenance to keep machines
up-to-date and services running smoothly. All scheduled maintenance will be announced in
[announcements mailing list](https://groups.google.com/a/watonomous.ca/g/watcloud-compute-cluster-announcements).
Emergency maintenance and maintenance that has little effect on user experience will be announced in the `#đŠ-watcloud-use` channel on Discord.

Sometimes, machines in the cluster may go down unexpectedly due to hardware failures or power outages.
We have a comprehensive suite of healthchecks and internal monitoring tools[^watcloud-observability] to detect these failures and notify us.
However, due to the part-time nature of the student team, we may not be able to respond to these failures immediately.
If you notice that a machine is down, please ping the WATcloud team on Discord
(`@WATcloud` or `@WATcloud Leads`, in the `#đŠ-watcloud-use` channel).

[^watcloud-observability]: Please refer to the [Observability](/docs/community-docs/watcloud/observability) page to learn more about the tools we use to monitor the cluster.

To see if a machine is having issues, please visit [status.watonomous.ca](https://status.watonomous.ca). The WATcloud team uses this page as
a dashboard to monitor the health of machines in the cluster.

## Usage Guidelines

- Use [SLURM](./slurm) as much as possible. SLURM streamlines resource allocation. You get a dedicated environment for your job, and you don't have to worry about CPU/memory contention.
- Be [nice](https://man7.org/linux/man-pages/man2/nice.2.html)
  - If you have a long-running non-interactive process on a general-use machine, please [increase its niceness](https://www.tecmint.com/set-linux-process-priority-using-nice-and-renice-commands/) so that interactive programs don't lag.
  - Being nice is simply changing `./my_program arg1 arg2{:bash}` to `nice ./my_program arg1 arg2{:bash}`.
- Clean up after yourself
  - If you are using `/mnt/scratch` on a general-use machine, please make sure to clean up your files after you are done with them.
  - Please only use `/home` for small files. Writing large files to `/home` will significantly slow down the filesystem for all users.
  - `/mnt/wato-drive*` are large storage pools, but they are not infinite and can fill up quickly with today's large datasets.
    Please remove unneeded files from these directories.
  - When using Docker on general-use machines, please clean up your Docker images and containers regularly. `docker system prune --all{:bash}` is your friend.

## Troubleshooting

This section contains some common issues that users may encounter when using machines in the cluster and their solutions. If you encounter an issue that is not listed here, please [reach out](./support-resources).

### Account not available

You may see the following message when trying to log into a general-use machine:

> This account is currently not available.

This message means that your account is locked.
This can happen if your account is expired and is pending deletion.
To re-enable your account, please reach out to your [WATcloud contact](/docs/services#watcloud-contact).

### Permission denied while trying to connect to the Docker daemon

You may encounter this error when trying to run Docker commands on general-use machines:

```
> docker ps
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json": dial unix /var/run/docker.sock: connect: permission denied
```

This error occurs when your shell doesn't source `/etc/profile` (`zsh` is known to do this). Our Docker rootless setup
requires that the `DOCKER_HOST` environment variable be set properly, which is done in `/etc/profile`. To fix this,
please add the following line to your shell's rc file (e.g. `~/.zshrc`):

```bash copy
export DOCKER_HOST=unix://${XDG_RUNTIME_DIR}/docker.sock
```

Remember to restart your shell or source the rc file after making the change.

### Disk quota exceeded when running Docker commands

You may encounter the following error when running Docker commands on general-use machines:

```
> docker pull hello-world
...
open /var/lib/cluster/users/$UID/docker/tmp/GetImageBlob3112047691: disk quota exceeded
```

This means that you have exceeded your allocated storage quota[^quota-more-info].
Here are some commands you can use to free up disk space[^docker-prune]:

```bash
# remove dangling images (images without tags)
docker image prune
# remove all images without an existing container
docker image prune --all
# remove stopped containers
docker container prune
# remove all volumes not used by at least one container
docker volume prune
# remove all stopped containers, dangling images, unused networks, and unused build cache
docker system prune
# same as above, but also removes unused volumes
docker system prune --volumes
# same as above, but also removes unused images
docker system prune --volumes --all
```

[^docker-prune]: For more information about the Docker prune commands, please refer to the [Docker manual](https://docs.docker.com/config/pruning/)
[^quota-more-info]: For more information about storage quotas, including how to check your current quota usage, please see the [Quotas](./quotas) page.


### Cannot connect to the Docker daemon

You may encounter this error when trying to run Docker commands on general-use machines:

```
> docker ps
Cannot connect to the Docker daemon at unix:///run/user/$UID/docker.sock. Is the docker daemon running?
```

To troubleshoot this issue, please run the following command to confirm whether the Docker daemon is running:

```bash copy
systemctl --user status docker-rootless
```

A healthy Docker daemon should look like this:

```ansi
[0;1;32mâ[0m docker-rootless.service - Docker Application Container Engine (Rootless)
     Loaded: loaded (/usr/lib/systemd/user/docker-rootless.service; enabled; vendor preset: enabled)
     Active: [0;1;32mactive (running)[0m since Tue 2024-01-16 05:47:04 UTC; 44s ago
       Docs: https://docs.docker.com/go/rootless/
   Main PID: 2090042 (rootlesskit)
      Tasks: 63
     Memory: 54.7M
        CPU: 587ms
     CGroup: /user.slice/user-1507.slice/user@1507.service/app.slice/docker-rootless.service
             ââ2090042 rootlesskit --net=slirp4netns --mtu=65520 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=builtin --copy-up=/etc --copy-up=/run --propagation=rslave /usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json
             ââ2090053 /proc/self/exe --net=slirp4netns --mtu=65520 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=builtin --copy-up=/etc --copy-up=/run --propagation=rslave /usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json
             ââ2090072 slirp4netns --mtu 65520 -r 3 --disable-host-loopback --enable-sandbox --enable-seccomp 2090053 tap0
             ââ2090080 dockerd --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json
             ââ2090112 containerd --config /run/user/1507/docker/containerd/containerd.toml
```

If the daemon failed to start, the output may look like this:

```ansi
[0;31;1mĂ[0m docker-rootless.service - Docker Application Container Engine (Rootless)
     Loaded: loaded (/usr/lib/systemd/user/docker-rootless.service; enabled; vendor preset: enabled)
     Active: [0;31;1mfailed[0m (Result: exit-code) since Tue 2024-01-16 05:15:38 UTC; 9min ago
       Docs: https://docs.docker.com/go/rootless/
    Process: 2049816 ExecStart=/usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json [0;31;1m(code=exited, status=1/FAILURE)[0m
   Main PID: 2049816 (code=exited, status=1/FAILURE)
        CPU: 283ms

Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: docker-rootless.service: Scheduled restart job, restart counter is at 3.
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: Stopped Docker Application Container Engine (Rootless).
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: [0;1;38:5:185mdocker-rootless.service: Start request repeated too quickly.[0m
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: [0;1;38:5:185mdocker-rootless.service: Failed with result 'exit-code'.[0m
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: [0;31;1mFailed to start Docker Application Container Engine (Rootless).
```

To get more information about why the daemon failed to start, you can view the logs by running:

```bash copy
journalctl --user --catalog --pager-end --unit docker-rootless.service
```

One of the most common reasons for the Docker daemon to fail to start is that the storage quota has been exceeded. If this is the case, you will see `disk quota exceeded` in the logs:

```ansi {3}
[0mJan 16 05:15:35 trpro-ubuntu2 dockerd-rootless.sh[2049885]: time="2024-01-16T05:15:35.066806684Z" level=info msg="containerd successfully booted in 0.020404s"
Jan 16 05:15:35 trpro-ubuntu2 dockerd-rootless.sh[2049852]: time="2024-01-16T05:15:35.077397993Z" level=info msg="stopping healthcheck following graceful shutdown" module=libcontainerd
Jan 16 05:15:36 trpro-ubuntu2 dockerd-rootless.sh[2049852]: failed to start daemon: Unable to get the TempDir under /var/lib/cluster/users/1507/docker: mkdir /var/lib/cluster/users/1507/docker/tmp: disk quota exceeded
Jan 16 05:15:36 trpro-ubuntu2 dockerd-rootless.sh[2049827]: [rootlesskit:child ] error: command [/usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json] exited: exit status 1
Jan 16 05:15:36 trpro-ubuntu2 dockerd-rootless.sh[2049816]: [rootlesskit:parent] error: child exited: exit status 1
Jan 16 05:15:36 trpro-ubuntu2 systemd[73045]: [0;1mdocker-rootless.service: Main process exited, code=exited, status=1/FAILURE[0m
[0;38:5:245mââ [0;32mSubject: Unit process exited[0m
[0;38:5:245mââ[0m [0;32mDefined-By: systemd[0m
[0;38:5:245mââ[0m [0;32mSupport: http://www.ubuntu.com/support[0m
[0;38:5:245mââ[0m
[0;38:5:245mââ[0m [0;32mAn ExecStart= process belonging to unit UNIT has exited.[0m
[0;38:5:245mââ[0m
[0;38:5:245mââ[0m [0;32mThe process' exit code is 'exited' and its exit status is 1.[0m
Jan 16 05:15:36 trpro-ubuntu2 systemd[73045]: [0;1;38:5:185mdocker-rootless.service: Failed with result 'exit-code'.
```

For more information about storage quotas, please see the [Quotas](./quotas) page.

To free up space for Docker, you can selectively delete files from the `/var/lib/cluster/users/$(id -u)/docker` directory until you are no longer over quota. However, it could be difficult to determine which files to delete. If you would like to delete all your Docker files (including images, containers, volumes, etc.), which effectively resets your Docker installation, you can delete the entire directory:

```bash copy
rootlesskit rm -r /var/lib/cluster/users/$(id -u)/docker
```

Here, `rootlesskit{:bash}` puts us in the "fake root" environment used by Docker rootless[^rootlesskit], which allows us to manage files owned by subuid/subgid.

[^rootlesskit]: Learn more about rootlesskit [here](https://github.com/rootless-containers/rootlesskit).

If you would prefer to use Docker tools to clean up your Docker installation (e.g. `docker system prune --all{:bash}`), you can reach out to a WATcloud team member to temporarily increase your storage quota so that Docker daemon can be started.

Once you have freed up enough space, you can restart the Docker daemon by running:

```bash copy
systemctl --user restart docker-rootless
```
</file>

<file path="docs/compute-cluster/overview.mdx">
# Overview (Deprecated)

We've moved this page to a [new location](./)! Please update your bookmarks. Redirecting in 5 seconds...

import Redirect from '@/components/redirect'

<Redirect to="./" delay_ms={5000} />
</file>

<file path="docs/compute-cluster/quotas.mdx">
import { GlobalQuotaTable, NodeLocalQuotaTable, CPURAMQuotaTable } from '@/components/quota-table'

# Quotas

To ensure that everyone has a fair share of resources, we enforce a set of quotas in the cluster.

## Terminology

- **Hard limit**: The limit that cannot be exceeded. If the hard limit for a disk quota is reached,
    the user will receive a "disk quota exceeded" error when trying to write to the disk.
- **Soft limit**: The limit that can be temporarily exceeded. We set the grace period to 7 days[^7-day-grace].
    If the soft limit is reached for more than 7 days, the user will receive a "disk quota exceeded"
    error when trying to write to the disk.

[^7-day-grace]: The 7-day grace period is the default on most Linux quota implementations.

## Disk quotas

### Default disk quotas

This section lists the default per-user disk quotas. These quotas are subject to change as we learn more about the usage patterns.

#### Global disk quotas

Global disk quotas are quotas on filesystems that are shared across all nodes.

<GlobalQuotaTable className="mt-4" />

#### Node-local disk quotas

Node-local disk quotas are quotas on filesystems that are local to each node.

<NodeLocalQuotaTable className="mt-4" />

### Checking your disk quota

The method for checking your disk quota depends on the filesystem type (`FS Type`).
The filesystem type is listed in the [default disk quotas](#default-disk-quotas) section.
This section describes how to check your disk quota for each filesystem type.

#### `ceph`

To check your disk quota, run the following command:

```bash copy
getfattr -n ceph.quota "<path>"
```

For example, to check the disk quota for your home directory, run the following command:

```bash copy
getfattr -n ceph.quota "$HOME"
```

In the output below, the user `ben` has a quota of 20 GiB and unlimited (represented by `0`) files in their home directory.

```
# file: home/ben
ceph.quota="max_bytes=21474836480 max_files=0"
```

To check your current usage, run the following commands:

```bash copy
getfattr -n ceph.dir.rbytes "<path>"
getfattr -n ceph.dir.rfiles "<path>"
```

For example, to check the current usage of your home directory, run the following commands:

```bash copy
getfattr -n ceph.dir.rbytes "$HOME"
getfattr -n ceph.dir.rfiles "$HOME"
```

In the output below, the user `ben` has used 16.8 GiB of space across 210,104 files in their home directory.

```
# file: home/ben
ceph.dir.rbytes="18056911629"

# file: home/ben
ceph.dir.rfiles="210104"
```

#### `xfs`

To check your disk quota, run the following command[^quota-command]:

 [^quota-command]: `quota` is a command-line utility that is available on most Linux installations with quota support. You can learn more about the command [here](https://linux.die.net/man/1/quota) or by running `man quota`

```bash copy
quota --human-readable=g,
```

For example, in the output below, the user `ben` has a soft limit of 30 GiB and a hard limit of 50 GiB on `/dev/sdc`, and has used 1 GiB of space.

```bash
> quota --human-readable=g,
Disk quotas for user ben (uid 1507):
     Filesystem   space   quota   limit   grace   files   quota   limit   grace
       /dev/sdc      1G     30G    50G              51       0       0
```

to find out which mountpoint corresponds to `/dev/sdc`, run the following command:

```bash
> df -h /dev/sdc
Filesystem      Size  Used Avail Use% Mounted on
/dev/sdc        200G   34G  167G  17% /var/lib/cluster
```

In this case, `/dev/sdc` is mounted on `/var/lib/cluster`. This drive is used to store docker-related data,
as shown in the [Default disk quotas](#default-disk-quotas) section.

### Requesting a disk quota increase

If you find that you regularly exceed your quotas, you can request for a quota increase by specifying your
desired quota in your profile and have your WATcloud contact[^watcloud-contact] approve it. You can edit your
profile using the [Profile Editor](../utilities/profile-editor).

[^watcloud-contact]: Your WATcloud contact is the person as described in the [Getting Access](./getting-access#determine-your-watcloud-contact) section.

## CPU and memory quotas

On general-use machines, per-user CPU and memory quotas are enforced to ensure fair resource sharing.

<CPURAMQuotaTable className="mt-4" />

Unlike disk quotas, we don't allow users to request CPU and memory quota increases. Please use [SLURM](./slurm) to run resource-intensive jobs.
</file>

<file path="docs/compute-cluster/slurm.mdx">
# SLURM

SLURM (Simple Linux Utility for Resource Management)[^slurm] is an open-source job scheduler that handles the allocation of resources in a compute cluster.
It is commonly used in HPC (High Performance Computing) environments. WATcloud uses SLURM to manage most of its compute resources.

[^slurm]: https://slurm.schedmd.com/

This page provides an introduction to SLURM and any WATcloud-specific details[^watcloud-specific] to get you started quickly.
For more advanced usage beyond the basics, please refer to the [official SLURM documentation](https://slurm.schedmd.com/).

[^watcloud-specific]: Some WATcloud-specific details include the available resources (e.g. how GPUs are requested using `shard` and `gpu` GRES),
    the temporary disk setup (requested using the `tmpdisk` GRES, mounted at `/tmp`), and software availability (e.g. docker rootless and Compute Canada CVMFS).

import { Callout } from 'nextra/components'

<Callout type="info">
WATcloud SLURM is currently in beta. If you encounter any issues, Please review the [troubleshooting](#troubleshooting) section
or [let us know](/docs/compute-cluster/support-resources).
</Callout>

## Terminology

Before we dive into the details, let's define some common terms used in SLURM:

- **SLURM Login node**: A node that users log into to submit jobs to the SLURM cluster. This is where you will interact with the SLURM cluster.
- **SLURM Compute node**: A node that runs jobs submitted to the SLURM cluster. This is where your job will run.
- **Partition**: A logical grouping of nodes in the SLURM cluster. Partitions can have different properties (e.g. different resource limits) and are used to organize resources.
- **Job**: A unit of work submitted to the SLURM cluster. A job can be interactive or batch.
- **Interactive job**: A job that runs interactively on a compute node. This is useful for debugging or running short tasks.
- **Batch job**: A job that runs non-interactively on a compute node. This is useful for running long-running tasks like simulations or ML training.
- **Job array**: A collection of jobs with similar parameters. This is useful for running parameter sweeps or other tasks that require running the same job multiple times with potentially different inputs.
- **Resource**: A physical or logical entity that can be allocated to a job. Examples include CPUs, memory, GPUs, and temporary disk space.
- **GRES (Generic Resource)**: A SLURM feature that allows for arbitrary resources to be allocated to jobs. Examples include GPUs and temporary disk space.

## Quick Start

### SSH into a SLURM login node

To submit jobs to the SLURM cluster, you must first SSH into one of the SLURM login nodes[^slurm-login-nodes]:

import { machineInfo } from '@/lib/data'

export function SLURMLoginNodes() {
  const slurmLoginNodes = machineInfo.machines.dev_vms.filter((machine) => machine.tags.map((tag) => tag.name).includes('SL'))
  return <ul>{slurmLoginNodes.map((machine) => <li key={machine.name}>{machine.name}</li>)}</ul>
}

<SLURMLoginNodes />

Instructions on how to SSH into machines can be found [here](./ssh).

[^slurm-login-nodes]: SLURM login nodes are also labelled `SL` in the [machine list](/machines#general-use-machines). These machines are subject to change. [Announcements](https://groups.google.com/a/watonomous.ca/g/watcloud-compute-cluster-announcements) will be made if changes occur.

### Interactive shell

Once SSHed into a SLURM login node, you can execute the following command to submit a simple job to the SLURM cluster:

```bash copy
srun --pty bash
```

This will start an interactive shell session on a compute node with the default resources.
You can view the resources allocated to your job by running:

```bash copy
scontrol show job $SLURM_JOB_ID
```

An example output is shown below:

```text {6,17}
JobId=1305 JobName=bash
   UserId=ben(1507) GroupId=ben(1507) MCS_label=N/A
   Priority=1 Nice=0 Account=watonomous-watcloud QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=0:0
   RunTime=00:00:04 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2024-03-16T06:39:57 EligibleTime=2024-03-16T06:39:57
   AccrueTime=Unknown
   StartTime=2024-03-16T06:39:57 EndTime=2024-03-16T07:09:57 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-03-16T06:39:57 Scheduler=Main
   Partition=compute AllocNode:Sid=10.1.100.128:1060621
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=wato2-slurm1
   BatchHost=wato2-slurm1
   NumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=1,mem=512M,node=1,billing=1,gres/tmpdisk=100
   AllocTRES=cpu=1,mem=512M,node=1,billing=1,gres/tmpdisk=100
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryCPU=512M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=bash
   WorkDir=/home/ben
   Power=
   TresPerNode=gres/tmpdisk:100
```

In this example, the job is allocated 1 CPU, 512MiB of memory, and 100MiB of temporary disk space (mounted at `/tmp`),
and is allowed to run for up to 30 minutes.

To request for more resources, you can use the `--cpus-per-task`, `--mem`, `--gres`, and `--time` flags.
For example, to request 4 CPUs, 4GiB of memory, 20GiB of temporary disk space, and 2 hours of running time, you can run:

```bash copy
srun --cpus-per-task 4 --mem 4G --gres tmpdisk:20480 --time 2:00:00 --pty bash
```

Note that the amount of requestable resources is limited by the resources available on the partition/node you are running on.
You can view the available resources by referring to the [View available resources](#view-available-resources) section.

### Cancelling a job

To cancel a job, you can use the `scancel` command.
You will need the job ID to cancel a job.
You can find the job ID by running `squeue`.
If you are in a job, you can also use the `$SLURM_JOB_ID` environment variable.

For example, you can see a list of your jobs by running:

```bash copy
squeue -u $(whoami)
```

Example output:

```text
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
4022   compute     bash      ben  R       0:03      1 thor-slurm1
```

To cancel the job with ID `4022`, you can run:

```bash copy
scancel 4022
```

### Using Docker

Unlike general use machines, the SLURM environment does not provide user-space systemd for managing background processes like the Docker daemon.
To use Docker, you will need to start the Docker daemon manually. We have provided a convenience script to do this:

```bash copy
slurm-start-dockerd.sh
```

If successful, you should see the following output:

```text
Dockerd started successfully!

Test it with:
docker run --rm hello-world
```

Note that `slurm-start-dockerd.sh` places the Docker data directory in `/tmp`.
You can request for more space using the `--gres tmpdisk:<size_in_MiB>` flag.


### Using GPUs

You can request access to GPUs by using the `--gres shard:<size_in_MiB>` flag. For example, if your workload requires 4 GiB of VRAM, you can run:

```bash copy
srun --gres shard:4096 --pty bash
```

Your job will be allocated a GPU with at least 4 GiB of unreserved VRAM.
Please note that the amount of VRAM requested is not enforced, and you should ensure that the amount requested is appropriate for your workload.

Using `shard` is the preferred way to request for GPU resources because it allows multiple jobs to share the same GPU.

It's common to request extra tmpdisk space along with GPUs. To do this, you can append `,tmpdisk:<size_in_MiB>` to the `--gres` flag. For example:

```bash copy
srun --gres shard:4096,tmpdisk:20480 --pty bash
```

If your workload requires exclusive access to a GPU, you can use the `--gres gpu` flag instead:

<Callout type="warning">Because the cluster is GPU-constrained, requesting whole GPUs is not recommended unless your workload can
make efficient use of the entire GPU.</Callout>

```bash copy
srun --gres gpu:1 --pty bash
```

This will allocate a whole GPU to your job. Note that this will prevent other jobs from using the GPU until your job is finished.

### Using CUDA

If your workload requires CUDA, you have a few options (not exhaustive):

#### Using the `nvidia/cuda` Docker image

You can use the `nvidia/cuda` Docker image to run CUDA workloads.
Assuming you have started the Docker daemon (see [Using Docker](#using-docker)), you can run the following command to start a CUDA container:

```bash copy
docker run --rm -it --gpus all -v $(pwd):/workspace nvidia/cuda:12.0.0-devel-ubuntu22.04 nvcc --version
```

Note that the version of the Docker image must be compatible with (usually this means lower than or equal to) the driver version installed on the compute node.
You can check the driver version by running `nvidia-smi`. If the driver version is not compatible with the Docker image, you will get an error that looks like this:

```text
> docker run --rm -it --gpus all -v $(pwd):/workspace nvidia/cuda:12.1.0-runtime-ubuntu22.04
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'
nvidia-container-cli: requirement error: unsatisfied condition: cuda>=12.1, please update your driver to a newer version, or use an earlier cuda container: unknown.
```

#### Using the Compute Canada CUDA module

The Compute Canada CVMFS[^cc-cvmfs] is mounted on the compute nodes. You can access CUDA by loading the appropriate module:

```bash
# Set up the module environment
source /cvmfs/soft.computecanada.ca/config/profile/bash.sh
# Load the appropriate environment
module load StdEnv/2023
# Load the CUDA module
module load cuda/12.2
# Check the nvcc version
nvcc --version
```

Compute Canada only provides select versions of CUDA, and does not provide an easy way to list all available versions.
A trick you can use is to run `which nvcc{:bash}` and trace back along the directory tree to find sibling directories
that contain other CUDA versions.

Note that the version of CUDA must be compatible with the driver version installed on the compute node.
You can check the driver version by running `nvidia-smi`.
You can find the CUDA compatibility matrix [here](https://docs.nvidia.com/deploy/cuda-compatibility/index.html).

[^cc-cvmfs]: The [Compute Canada CVMFS](https://docs.alliancecan.ca/wiki/Accessing_CVMFS) is mounted at `/cvmfs/soft.computecanada.ca` on the compute nodes. It provides access to a wide variety of software via [Lmod modules](https://docs.alliancecan.ca/wiki/Utiliser_des_modules/en).

### Batch jobs

The real power of SLURM comes from batch jobs.
Batch jobs are non-interactive jobs that start automatically when resources are available and release the resources when the job is finished.
This helps to maximize resource utilization and allows you to easily run large numbers of jobs (e.g. parameter sweeps).

To submit a batch job, create a script that looks like this:

```bash copy filename="slurm_job.sh"
#!/bin/bash
#SBATCH --job-name=my_job
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --gres tmpdisk:1024
#SBATCH --time=00:10:00
#SBATCH --output=logs/%j-%x.out  # %j: job ID, %x: job name. Reference: https://slurm.schedmd.com/sbatch.html#lbAH

echo "Hello, world! I'm running on $(hostname)"
echo "Counting to 60..."
for i in $(seq 60); do
    echo $i
    sleep 1
done
echo "Done!"
```

The `#SBATCH` lines are SLURM directives that specify the resources required by the job[^sbatch].
They are the same as the flags you would pass to `srun`.

To submit the job, run:

```bash copy
sbatch slurm_job.sh
```

This submits the job to the SLURM cluster, and you will receive a job ID in return.
After the job is submitted, it will be queued until resources are available.

You can see a list of your queued and in-progress jobs by running[^squeue]:

```bash copy
squeue -u $(whoami) --format="%.18i %.9P %.30j %.20u %.10T %.10M %.9l %.6D %R"
```

After the job starts, the output of the job is written to the file specified in the `--output` directive.
In the example above, you can view the output of the job by running:

```bash copy
tail -f logs/*-my_job.out
```

After the job finishes, it disappears from the queue.
You can retrieve useful information about the job (exit status, running time, etc.) by running[^sacct]:

```bash copy
sacct --format=JobID,JobName,State,ExitCode
```

[^sbatch]: `sbatch` is used to submit batch jobs to the SLURM cluster. For a full list of SLURM directives for `sbatch`, see the [sbatch documentation](https://slurm.schedmd.com/sbatch.html).
[^squeue]: `squeue` displays information about jobs in the queue. For a full list of formatting options, see the [squeue documentation](https://slurm.schedmd.com/squeue.html#OPT_format).
[^sacct]: `sacct` displays accounting data for jobs and job steps. For more information, see the [sacct documentation](https://slurm.schedmd.com/sacct.html).

#### Job arrays

Job arrays are a way to submit multiple jobs with similar parameters.
This is useful for running parameter sweeps or other tasks that require running the same job multiple times with potentially different inputs.

To submit a job array, create a script that looks like this:

```bash copy filename="slurm_job_array.sh" {7-8,10}
#!/bin/bash
#SBATCH --job-name=my_job_array
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --gres tmpdisk:1024
#SBATCH --time=00:10:00
#SBATCH --output=logs/%A-%a-%x.out # %A: job array master job allocation number, %a: Job array index, %x: job name. Reference: https://slurm.schedmd.com/sbatch.html#lbAH
#SBATCH --array=1-10

echo "Hello, world! I'm job $SLURM_ARRAY_TASK_ID, running on $(hostname)"
echo "Counting to 60..."
for i in $(seq 60); do
    echo $i
    sleep 1
done
echo "Done!"
```

The `--array` directive specifies the range of the job array (in this case, from 1 to 10, inclusive).

To submit the job array, run:

```bash copy
sbatch slurm_job_array.sh
```

This will submit 10 jobs with IDs ranging from 1 to 10.
You can view the status of the job array by running:

```bash copy
squeue -u $(whoami) --format="%.18i %.9P %.30j %.20u %.10T %.10M %.9l %.6D %R"
```

After jobs in the array start, the output of each job is written to a file specified in the `--output` directive.
In the example above, you can view the output of each job by running:

```bash copy
tail -f logs/*-my_job_array.out
```

To learn more about job arrays, including environment variables available to job array scripts,
see the [official documentation](https://slurm.schedmd.com/job_array.html).

#### Long-running jobs

Each job submitted to the SLURM cluster has a time limit.
The time limit can be set using the `--time` directive.
The maximum time limit is determined by the partition you are running on.
You can view a list of partitions, including the default partition, by running `sinfo`[^view-available-resources]:

```text
> sinfo
PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute*         up 1-00:00:00      5   idle thor-slurm1,tr-slurm1,trpro-slurm[1-2],wato2-slurm1
compute_dense    up 7-00:00:00      5   idle thor-slurm1,tr-slurm1,trpro-slurm[1-2],wato2-slurm1
```

In the output above, the cluster has 2 partitions, `compute` (default) and `compute_dense`, with time limits of 1 day and 7 days, respectively.
If your job requires more than the maximum time limit for the default partition, you can specify a different partition using the `--partition` flag.
For example:

```bash copy filename="slurm_compute_dense_partition.sh"
#!/bin/bash
#SBATCH --job-name=my_dense_job
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --gres tmpdisk:1024
#SBATCH --partition=compute_dense
#SBATCH --time=2-00:00:00
#SBATCH --output=logs/%j-%x.out  # %j: job ID, %x: job name. Reference: https://slurm.schedmd.com/sbatch.html#lbAH

echo "Hello, world! I'm allowed to run for 2 days!"
for i in $(seq $((60*60*24*2))); do
    echo $i
    sleep 1
done
echo "Done!"
```

If you require a time limit greater than the maximum time limit for any partition, please [contact the WATcloud team](./support-resources) to request an exception.

[^view-available-resources]: For more information on viewing available resources, see the [View available resources](#view-available-resources) section.

## Extra details

### SLURM v.s. general-use machines

The SLURM environment is configured to be as close to the general-use environment as possible.
All of the same network drives and software are available. However, there are some differences:

- The SLURM environment uses a `/tmp` drive for temporary storage instead of `/mnt/scratch` on general-use machines.
    Temporary storage can be requested using the `--gres tmpdisk:<size_in_MiB>` flag.
- The SLURM environment does not have a user-space systemd for managing background processes like the Docker daemon.
    Please follow the instructions in the [Using Docker](#using-docker) section to start the Docker daemon.

### View available resources

There are a few ways to view the available resources on the SLURM cluster:

#### View a summary of available resources

```bash copy
sinfo
```

Example output:

```text
PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute*         up 1-00:00:00      5   idle thor-slurm1,tr-slurm1,trpro-slurm[1-2],wato2-slurm1
compute_dense    up 7-00:00:00      5   idle thor-slurm1,tr-slurm1,trpro-slurm[1-2],wato2-slurm1
```

#### View available partitions

```bash copy
scontrol show partitions
```

Example output:

```text
PartitionName=compute
   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
   AllocNodes=ALL Default=YES QoS=N/A
   DefaultTime=00:30:00 DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO
   MaxNodes=UNLIMITED MaxTime=1-00:00:00 MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED MaxCPUsPerSocket=UNLIMITED
   Nodes=thor-slurm1,tr-slurm1,trpro-slurm[1-2],wato2-slurm1
   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO
   OverTimeLimit=NONE PreemptMode=OFF
   State=UP TotalCPUs=240 TotalNodes=5 SelectTypeParameters=NONE
   JobDefaults=(null)
   DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED
   TRES=cpu=233,mem=707441M,node=5,billing=233,gres/gpu=10,gres/shard=216040,gres/tmpdisk=921600

PartitionName=compute_dense
   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
   AllocNodes=ALL Default=NO QoS=N/A
   DefaultTime=00:30:00 DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO
   MaxNodes=UNLIMITED MaxTime=7-00:00:00 MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED MaxCPUsPerSocket=UNLIMITED
   Nodes=thor-slurm1,tr-slurm1,trpro-slurm[1-2],wato2-slurm1
   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO
   OverTimeLimit=NONE PreemptMode=OFF
   State=UP TotalCPUs=240 TotalNodes=5 SelectTypeParameters=NONE
   JobDefaults=(null)
   DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED
   TRES=cpu=233,mem=707441M,node=5,billing=233,gres/gpu=10,gres/shard=216040,gres/tmpdisk=921600
```

#### View available nodes

```bash copy
scontrol show nodes
```

Example output:

```text
NodeName=trpro-slurm1 Arch=x86_64 CoresPerSocket=1 
   CPUAlloc=0 CPUEfctv=98 CPUTot=100 CPULoad=0.06
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=gpu:rtx_3090:4(S:0),shard:rtx_3090:96K(S:0),tmpdisk:300K
   NodeAddr=trpro-slurm1.cluster.watonomous.ca NodeHostName=trpro-slurm1 Version=23.11.4
   OS=Linux 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 
   RealMemory=423020 AllocMem=0 FreeMem=419161 Sockets=100 Boards=1
   CoreSpecCount=2 CPUSpecList=98-99 MemSpecLimit=2048
   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=compute 
   BootTime=2024-03-24T00:17:08 SlurmdStartTime=2024-03-24T02:27:46
   LastBusyTime=2024-03-24T02:27:46 ResumeAfterTime=None
   CfgTRES=cpu=98,mem=423020M,billing=98,gres/gpu=4,gres/shard=98304,gres/tmpdisk=307200
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/a ExtSensorsWatts=0 ExtSensorsTemp=n/a

...
```

In this example, the node `trpro-slurm1` has the following allocable resources:
98 CPUs, around 413 GiB of RAM, 4 RTX 3090 GPUs, 98304 MiB of VRAM, and 300GiB of temporary disk space.

### GRES

GRES (Generic Resource)[^gres] is a SLURM feature that allows for arbitrary resources to be allocated to jobs.
The WATcloud cluster provides the following GRES:

[^gres]: https://slurm.schedmd.com/gres.html

#### `gres/tmpdisk`

`tmpdisk` is a GRES that represents temporary disk space. This resource is provisioned using a combination
of `job_container/tmpfs`[^job-container-tmpfs] and custom scripts. The temporary disk space is mounted at `/tmp` and is automatically
cleaned up when the job finishes. You can request for temporary disk space using the `--gres tmpdisk:<size_in_MiB>` flag.
Below is an example:

```bash
# Request 1 GiB of temporary disk space
srun --gres tmpdisk:1024 --pty bash
```

To see the total amount of temporary disk space available on a node, please refer to the [View available resources](#view-available-resources) section.

[^job-container-tmpfs]: https://slurm.schedmd.com/job_container_tmpfs.html

#### `gres/shard` and `gres/gpu`

`shard` and `gpu` are GRES that represent GPU resources.
Allocation of these resources is managed by built-in SLURM plugins that interface with various GPU libraries.

The `shard` GRES is used to request access to a portion of a GPU.
In the WATcloud cluster, the amount of allocable `shard` equals the amount of VRAM (in MiB) on each GPU.
This representation is chosen because it is a concrete metric that works across different GPU models.
The amount of resources requested using `shard` is not enforced, so please ensure that the `shard` requested is appropriate for your workload.
To request for `shard`, use the `--gres shard[:type]:<size_in_MiB>`[^shard-restriction] flag, where `type` is optional and can be used
to specify a specific GPU type. Below are some examples:

```bash
# Request 2 GiB of VRAM on any available GPU
srun --gres shard:2048 --pty bash

# Request 4 GiB of VRAM on an RTX 3090 GPU
srun --gres shard:rtx_3090:4096 --pty bash
```

To see a list of available GPU types, please refer to the [View available resources](#view-available-resources) section.

[^shard-restriction]: Note that `size_in_MiB` must not exceed the amount of VRAM on a single GPU
(can be determined by dividing the amount of `shard` available on a node by the number of GPUs on that node).
If you require more VRAM than a single GPU can provide, please use the `--gres gpu` flag instead (see below).

The `gpu` GRES is used to request exclusive access to GPUs[^gpu-management].
This is not recommended unless your workload can make efficient use of the entire GPU.
If you are unsure, please use the `shard` GRES instead.
To request for `gpu`, use the `--gres gpu[:type]:<number_of_gpus>` flag, where `type` is optional and can be used
to specify a specific GPU type. Below are some examples:

```bash
# Request access to any available GPU
srun --gres gpu:1 --pty bash

# Request access to a whole RTX 3090 GPU
srun --gres gpu:rtx_3090:1 --pty bash
```

To see a list of available GPU types, please refer to the [View available resources](#view-available-resources) section.

[^gpu-management]: For more information on GPU management, please refer to the [GPU Management](https://slurm.schedmd.com/gres.html#GPU_Management) SLURM documentation.

#### Requesting multiple GRES

You can request multiple GRES by separating them with a comma. For example, to request 1 GiB of `shard` and 2 GiB of `tmpdisk`, you can run:

```bash copy
srun --gres shard:1024,tmpdisk:2048 --pty bash
```

### CVMFS

CVMFS (CernVM File System)[^cvmfs] is a software distribution system that is widely adopted in the HPC community.
It provides a way to distribute software to compute nodes without having to install them on the nodes themselves.

We make use of the [Compute Canada CVMFS](https://docs.alliancecan.ca/wiki/Accessing_CVMFS) to provide access to software available on Compute Canada clusters.
For example, you can access CUDA by loading the appropriate module (see [Using CUDA](#using-cuda)).
A list of all available modules can be found via the [official documentation](https://docs.alliancecan.ca/wiki/Available_software).

[^cvmfs]: https://cvmfs.readthedocs.io/en/stable/

## Troubleshooting

### Invalid Account

You may encounter the following error when trying to submit a job:

```text
srun: error: Unable to allocate resources: Invalid account or account/partition combination specified
```

This error usually occurs when your user does not have an associated account. You can verify this by running:

```bash copy
sacctmgr show user $(whoami)
```

If the output is empty (as shown below), then you do not have an associated account.

```text
      User   Def Acct     Admin
---------- ---------- ---------
```

A common reason for this is that your WATcloud profile is not associated with a registered affiliation[^registered-affiliation].
You can confirm this by requesting a copy of your profile using the [Profile Editor](/docs/utilities/profile-editor) and
checking whether the `general.affiliations` field contains a registered affiliation.
If your affiliation is not registered, please have your group lead fill out the [registration form](/docs/registered-affiliations#registering-a-group).

[^registered-affiliation]: Registered affiliations are distinct from "legacy" affiliations[^legacy-affiliation]. More information
    about registered affiliations can be found [here](/docs/registered-affiliations).
[^legacy-affiliation]: Legacy affiliations have the prefix `[Legacy]`. We don't have sufficient information from these affiliations
    to support them in the SLURM environment.
</file>

<file path="docs/compute-cluster/ssh.mdx">
import { Callout } from 'nextra/components'

# SSH

SSH stands for Secure Shell. It is a commonly used protocol that allows you to connect to a remote machine and execute commands on it.
Learn more about SSH [here](https://www.cloudflare.com/learning/access-management/what-is-ssh/).

<Callout type="warning">
Before proceeding, please make sure that you are a registered user of the WATcloud compute cluster. Not registered? Make a request [here](./getting-access)).
</Callout>

## Quick Start

Here's a tool to help you generate a personalized SSH command. This tool will generate a command that you can copy and paste into your terminal.
Note that the commands generated by this tool are only tested on Linux and macOS. 

### Command Generator

Choose your preferred machine and fill in the fields below to obtain personalized SSH commands.

import { SSHCommandGenerator } from '@/components/ssh-command-generator'
import { Separator } from "@/components/ui/separator"

<div className="my-6 p-4 ring-1 ring-ring rounded-md">
    <SSHCommandGenerator />
</div>

The generated commands do *not* require setting up ssh agent[^ssh-agent] or ssh config[^ssh-config].
However, you may soon find that setting them up will make your life easier. If you are interested in learning more about these
tools, please check out [Tips and Tricks](#tips-and-tricks) and the official documentation linked in the footnotes.

[^ssh-agent]: [SSH agent](https://www.ssh.com/ssh/agent) is a program that runs in the background and stores your SSH keys.
It comes with neat features like [SSH agent forwarding](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/using-ssh-agent-forwarding)
that allows you to use your local SSH keys on remote machines.

[^ssh-config]: [SSH config](https://www.ssh.com/ssh/config/) is a configuration file that allows you to simplify your SSH commands.

## Syntax

The general syntax for connecting to the cluster is:

```bash copy
SSH_KEY_PATH="<path_to_ssh_key>"
SSH_USERNAME="<username>"
SSH_HOST="<hostname>"
ssh -v -i "$SSH_KEY_PATH" "$SSH_USERNAME@$SSH_HOST"
```

or if you are using a jump host:

```bash copy
SSH_KEY_PATH="<path_to_ssh_key>"
SSH_USERNAME="<username>"
SSH_HOST="<hostname>"
SSH_JUMP_HOST="<jump_host>"
ssh -v -o ProxyCommand="ssh -W %h:%p -i \"$SSH_KEY_PATH\" \"$SSH_USERNAME@$SSH_JUMP_HOST\"" -i "$SSH_KEY_PATH" "$SSH_USERNAME@$SSH_HOST"
```

## Tips and Tricks

### Additional SSH Keys

To use additional SSH keys, you have the following options:

1. [Update your profile](../utilities/profile-editor) to add additional SSH keys.
2. Manually create `~/.ssh/authorized_keys` and place your SSH keys in there.You'll
need to make sure that this file is present in all machines that you want to connect to[^authorized_keys_note].

[^authorized_keys_note]: For example, if you use Bastion to connect to the cluster, you'll need to make sure that
`~/.ssh/authorized_keys` is present on both Bastion and the machine you intend to connect to.

### SSH Agent

SSH agent is useful for many reasons, for example:
1. It removes the need to use the `-i` flag to specify the path to your SSH key.
2. It allows you to use [SSH agent forwarding](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/using-ssh-agent-forwarding).

You can use SSH agent as follows:

```bash copy
# Start an SSH agent if it's not already running
[[ -z "$SSH_AUTH_SOCK" ]] && eval "$(ssh-agent -s)"
# Add your SSH key to the agent. Replace ~/.ssh/id_rsa with the path to your SSH key.
ssh-add ~/.ssh/id_rsa
# Connect to the cluster
ssh -v "<ssh_username>@<ssh_host>" # This is the same as ssh -v -i ~/.ssh/id_rsa "<ssh_username>@<ssh_host>" without SSH agent
# or if you are using a jump host
ssh -v -o ProxyCommand="ssh -W %h:%p \"<ssh_username>@<ssh_jump_host>\"" "<ssh_username>@<ssh_host>"
```

### `~/.ssh/config`

If you find yourself using the same SSH command over and over again, you can use `~/.ssh/config` to simplify your life.

For example, if you find yourself using the following command often:

```bash copy
ssh -v -o ProxyCommand="ssh -W %h:%p -i \"<ssh_key_path>\" \"<ssh_username>@<ssh_jump_host>\"" -i "<ssh_key_path>" "<ssh_username>@<ssh_host>"
```

you can add the following to your `~/.ssh/config`:

```ssh-config copy
Host <ssh_jump_host>
    HostName <ssh_jump_host>
    User <ssh_username>
    IdentityFile <ssh_key_path>

Host <ssh_host>
    HostName <ssh_host>
    User <ssh_username>
    IdentityFile <ssh_key_path>
    ProxyJump <ssh_jump_host>
```

and then simply run:

```bash copy
ssh -v <ssh_host>
```

A real-world example of this is:

```ssh-config copy
Host bastion
    HostName bastion.watonomous.ca
    User alex
    IdentityFile ~/.ssh/id_rsa

Host derek3-ubuntu2
    HostName derek3-ubuntu2.cluster.watonomous.ca
    User alex
    IdentityFile ~/.ssh/id_rsa
    ProxyJump bastion
```

```bash copy
ssh -v derek3-ubuntu2
```
</file>

<file path="docs/compute-cluster/support-resources.mdx">
# Support Resources

import websiteConfig from '@/build/fixtures/website-config.json'
import { Link } from 'nextra-theme-docs'

Having trouble accessing the compute resources? Exhausted all the documentation and still can't figure it out?
We are here to help! Please reach out to us via one of the following methods (in order of preference):

- <Link href={`https://discord.gg/${websiteConfig.discord_invite_code}`}>Discord</Link> (`#ask-ai` channel for instant AI answers, `#🌩-watcloud-use` channel for general discussions)
- infra-outreach@watonomous.ca
</file>

<file path="docs/utilities/_meta.json">
{
  "*": {
    "theme": {
      "timestamp": false
    }
  }
}
</file>

<file path="docs/utilities/assets.mdx">
# Assets

WATcloud Assets is a system for managing files[^example-uses].
This page contains tools for interacting with the assets system.

[^example-uses]: Some use cases are storing files that are too large for git (e.g. images, videos, etc.), or files that need to be shared between multiple projects.

import { AssetUploader, AssetInspector } from '@/components/assets'

## Inspector

The inspector is used to get information about assets.

<div className="mt-6">
    <AssetInspector />
</div>

## Uploader

The uploader is a tool for uploading assets.

<div className="mt-6">
    <AssetUploader />
</div>
</file>

<file path="docs/utilities/github.mdx">
# GitHub Utilities

## Username to Global Node ID Converter

This tool uses the GitHub API to convert usernames to [global node IDs](https://docs.github.com/en/graphql/guides/using-global-node-ids). This is useful for persisting GitHub user data in a database, as usernames can be changed and global node IDas are immutable.

import { UsernameToID } from '@/components/github'

<div className="mt-6">
    <UsernameToID />
</div>
</file>

<file path="docs/utilities/onboarding-form.mdx">
import { Callout } from 'nextra/components'

# Onboarding Form

Use the form below to request access to WATcloud services.

Please select the services that you need access to and fill out the required information.
If you were not told which services to request, please clarify with your WATcloud contact[^watocloud-contact].
Requesting access to services that you do
not need will slow down the approval process.

<Callout type="info">
**Editing an existing profile?** Please use the [profile editor](./profile-editor) to request for a pre-populated form.
</Callout>

[^watocloud-contact]: Your WATcloud contact is usually your group lead.
    See [Getting Access](/docs/compute-cluster/getting-access) for more information.
	
import OnboardingForm from '@/components/onboarding-form';

<OnboardingForm />
</file>

<file path="docs/utilities/profile-editor.mdx">
# Profile Editor

If you'd like to update your WATcloud profile, please use the form below.

import { ProfileEditor } from '@/components/profile-editor'

<div className="mt-6">
    <ProfileEditor />
</div>
</file>

<file path="docs/_meta.json">
{
  "index": "Introduction",
  "services": "Services",
  "compute-cluster": "Compute Cluster",
  "registered-affiliations": "Registered Affiliations",
  "utilities": "Utilities",
  "community-docs": "Community Docs"
}
</file>

<file path="docs/community-docs.mdx">
# Community Docs

This section contains documentation not directly related to the services offered by WATcloud. This includes
group-specific training materials, guides, and other resources. This section is maintained by the corresponding groups.

import PageIndex from '@/components/page-index'

<PageIndex pageRoot="/docs/community-docs" />
</file>

<file path="docs/compute-cluster.mdx">
# WATcloud Compute Cluster

import Picture from '@/components/picture'
import { ServerRoomLight } from '@/build/fixtures/images'

<Picture alt="Server Room (Light)" image={ServerRoomLight} />

The WATcloud Compute Cluster is a collection of servers that are available for use by students and faculty at the University of Waterloo
and partnering institutions.

## Quick Links

import {
  CpuIcon,
  CheckSquareIcon,
  TerminalIcon,
  BookMarkedIcon,
  HelpCircleIcon,
  ServerIcon,
} from "lucide-react"
import { Card, Cards } from 'nextra/components'

<Cards>
  <Card icon={<CpuIcon />} title="Machine List" href="/machines" />
  <Card icon={<CheckSquareIcon />} title="Getting Access" href="./compute-cluster/getting-access" />
  <Card icon={<BookMarkedIcon />} title="Machine Usage Guide" href="./compute-cluster/machine-usage-guide" />
  <Card icon={<TerminalIcon />} title="SSH Guide" href="./compute-cluster/ssh" />
  <Card icon={<ServerIcon />} title="SLURM Guide" href="./compute-cluster/slurm" />
  <Card icon={<HelpCircleIcon />} title="Support Resources" href="./compute-cluster/support-resources" />
</Cards>
</file>

<file path="docs/index.mdx">
---
title: "Introduction"
---

# WATcloud

import { Link } from 'nextra-theme-docs'
import websiteConfig from '@/build/fixtures/website-config.json'

WATcloud is a student-run cloud infrastructure group at the University of Waterloo.
Our main service is the [WATcloud compute cluster](/docs/compute-cluster) (sometimes referred to as "the compute cluster" or "WATcloud"),
where we provide compute (CPU and GPU), storage, and networking resources to student teams, clubs, and labs at the university and partnering organizations.

{/*
TODO: Once we have the affiliation infrastructure set up, list the number of student teams we help, the number of FYDP groups, and the total number of users.
We could list total users and the number of users in the past term and the current term.
*/}

## Quickstart

import { Cards } from 'nextra/components'
import Picture from '@/components/picture'
import {
  ComputerDark,
  ComputerLight,
  RobotDark,
  RobotLight,
  CloudDark,
  CloudLight,
} from '@/build/fixtures/images'

<Cards num={3}>
  <Cards.Card image arrow title="Get Access" href="/docs/compute-cluster/getting-access">
    <span className="hidden dark:block"><Picture alt="Abstract Computer (Dark)" image={ComputerDark} /></span>
    <span className="dark:hidden"><Picture alt="Abstract Computer (Light)" image={ComputerLight} /></span>
  </Cards.Card>
  <Cards.Card image arrow title="View Specs" href="/machines">
    <span className="hidden dark:block"><Picture alt="Robot with GPU (Dark)" image={RobotDark} /></span>
    <span className="dark:hidden"><Picture alt="Robot with GPU (Light)" image={RobotLight} /></span>
  </Cards.Card>
  <Cards.Card image arrow title="Get Involved" href="/get-involved">
    <span className="hidden dark:block"><Picture alt="Constructing the Cloud (Dark)" image={CloudDark} /></span>
    <span className="dark:hidden"><Picture alt="Constructing the Cloud (Light)" image={CloudLight} /></span>
  </Cards.Card>
</Cards>

## Brief History

WATcloud started as a group of computing enthusiasts who build servers at the [WATonomous][wato] student design team. Now we serve clubs
and teams from all over the University of Waterloo and support collaborative projects with international schools like [MIT][mit],
[Pitt][pitt], [RIT][rit], [CMU][cmu], and [PoliTO][polito][^international-collaboration].

[^international-collaboration]: An example of an international group we support is MIT-PITT-RW (MPRW), a collaboration between UWaterloo,
MIT, Pitt, and RIT to build a full-size self-driving racecar. UWaterloo students work alongside our international collaborators to use the
WATcloud compute cluster to develop software, run simulations, and train ML models.

[wato]: https://watonomous.ca
[mit]: https://mit.edu
[pitt]: https://pitt.edu
[rit]: https://rit.edu
[cmu]: https://cmu.edu
[polito]: https://www.polito.it/


## Join Our Community
For real-time server status updates and getting help, join us in our <Link href={`https://discord.gg/${websiteConfig.discord_invite_code}`}>Discord server</Link>. And stay tuned for our upcoming workshops and training sessions!

## FAQ

### Is the compute cluster free to use?

Absolutely! We're a student-run group, and we don't charge for our infrastructure. However, we do have a limited amount of resources, so we ask that you use them responsibly.

### How do I get access to the compute cluster?

Please visit the [Getting Access](/docs/compute-cluster/getting-access) page for more information.

### What’s the relationship between WATcloud and WATonomous?

WATonomous' compute cluster was born to meet the needs of the team. As the team grew, so did the cluster. Eventually, the subteam responsible for the cluster
became WATcloud, and opened up the cluster up to other groups. Now, WATcloud and WATonomous operate akin to AWS and Amazon.
We work in close proximity (e.g. we share the same Discord server and Google Workspace), but the services we provide are tailored to all groups we support.
WATonomous has access to the same [support resources](/docs/compute-cluster/support-resources) as every other group.
</file>

<file path="docs/registered-affiliations.mdx">
# Registered Affiliations

import { Callout } from 'nextra/components'

WATcloud Registered Affiliations are groups whose members can request for WATcloud compute cluster access.
If you are a group lead, please register your group using the [form](#registering-a-group) below.

## Registering a group

Please use the form below to register your group. We will review your request and get back to you
with the next steps.

import AffiliationForm from "@/components/affiliation-form"

<AffiliationForm />

## History

When WATcloud first opened up access to non-WATonomous groups, we asked group leads to provide and maintain a list of their members who required access to WATcloud services. We then assigned a WATcloud member to each group to act as a liaison and approve access requests. This approach worked reasonably well initially, but as the number of groups increased, it became challenging to keep track of group memberships and the individuals responsible for approving access requests. Additionally, we received requests from groups seeking greater visibility into the provisioning process so that they could resolve issues (e.g., incorrect GitHub usernames) themselves.

To address these challenges, we created a new system for managing access to WATcloud services, which we call "Registered Affiliations." The system provides access to and trains a few members from each group to manage WATcloud services for their group. They become the point of contact for their members and have visibility into the provisioning process. This new approach allows us to scale up the number of groups with minimal overhead.
</file>

<file path="docs/services.mdx">
# Services

WATcloud offers a variety of services. This page provides an overview of the services available.

## General Access instructions

Access to most[^access-otherwise] services listed below can be requested via the [onboarding form](/docs/utilities/onboarding-form).
Requests will be reviewed by your group's WATcloud contact.

[^access-otherwise]: If a service requires a different access process, it will be mentioned in the service description.

### WATcloud contact

A WATcloud contact is a designated person within each group (usually the group lead) who is responsible for approving access request and assisting users during the onboarding process.
The following groups are registered with WATcloud:

import { affiliationInfo } from '@/lib/data'
import { AffiliationList } from '@/components/affiliation-list'

<AffiliationList affiliationInfo={affiliationInfo} />

If your group is not listed or shows up as `[Legacy]`[^legacy-affiliation], please ask your group lead to [register the group][registered-affiliations] with WATcloud.

[registered-affiliations]: /docs/registered-affiliations
[^legacy-affiliation]: A legacy affiliation is a group that was onboarded before the current [registered affiliations][registered-affiliations] system was in place. Members of legacy affiliations can still access the compute cluster, but in order to use newer features like [SLURM](/docs/compute-cluster/slurm), the group must be registered.

## Service Directory

import userSchemaJSON from "@/build/fixtures/user.schema.generated.json";
import { lookupStringMDX, userSchemaStrings } from "@/lib/data";

export function ServiceDescriptions() {
    const ret = [];
    for (const [serviceKey, service] of Object.entries(userSchemaJSON.properties)) {
        if (service.properties?.enabled === undefined) {
            // only list services that can be enabled/disabled
            continue;
        }
        if (!service["$services_description"]) {
            throw new Error(`Service ${serviceKey} is missing a "$services_description" field`);
        }

        const Description = lookupStringMDX(userSchemaStrings, service["$services_description"]);

        ret.push(
            <div key={serviceKey}>
                <h3>{service.title}</h3>
                <Description />
            </div>
        );
    }
    return ret;
}

<ServiceDescriptions />
</file>

<file path="get-involved/_meta.json">
{
    "index": "Introduction",
    "sponsor": "Sponsor",
    "join": "Join WATcloud"
}
</file>

<file path="get-involved/index.mdx">
# Get Involved

The services provided by WATcloud would not be possible without the help of our sponsors and volunteers. If you'd like to support us in our mission to make compute accessible, please consider [sponsoring](./get-involved/sponsor) or [joining our team](./get-involved/join).
</file>

<file path="get-involved/join.mdx">
# Joining WATcloud

- Automation
- DevOps
- Sysadmin
- HPC
- Linux
- Terraform
- Ansible
- Infrastructure as Code
- CI/CD
- Observability
- Kubernetes
- SLURM
- HomeLab
- Web Development
- Robots taking over the world

Does any of the above sound familiar? Are you interested in bringing powerful compute to the masses?
Do you want to work with compute infrastructure similar to those used by the world's most well-known companies[^tesla]?
If so, we'd love to have you onboard!

[^tesla]: Our compute infrastructure is errily similar to the dev farm used by the Tesla Autopilot team 😱.

## Who we're looking for

WATcloud is not like a course project, where you can do some work, get a grade, and then forget about it.
We provide a service that is always up. We have users that depend on us.
We have a responsibility to keep our service running, and to keep our users happy.
We're looking for people who are passionate about what they do, and who are willing to put in the effort to build and quickly iterate on projects until every aspect is fully automated, reliable, observable, and trivially maintainable[^e2e-maintainable].
Please take a look at our [guidelines](/docs/community-docs/watcloud/guidelines) to get a sense of what we expect from our team members.

[^e2e-maintainable]: A project is trivially maintainable if it can be maintained by someone who has never seen the project before, and who has no prior knowledge of the project's internals beyond a high-level overview of its purpose. Most of the time, this involves building something that we can take down and rebuild from scratch by running a single command.

## How to apply

import { Link } from 'nextra-theme-docs'
import websiteConfig from '@/build/fixtures/website-config.json'

The best way to join WATcloud is to start contributing! We have a backlog of projects that we'd like to work on, but the list of projects always grows faster than we can work on them. A few of our projects are self-contained enough that anyone can pick them up and work on them. Below is a list of such projects. If you are able to complete one of these projects, we will be happy to bring you onboard immediately! If you are interested in working on a project that is not listed below, such as hardware projects (e.g. building computers, upgrading networking hardware) and projects that directly affect our infrastructure (Kubernetes, Terraform, Ansible, etc.), please reach out to us on <Link href={`https://discord.gg/${websiteConfig.discord_invite_code}`}>Discord</Link> or email infra-outreach@watonomous.ca.

When working on projects, please adhere to the [guidelines](/docs/community-docs/watcloud/guidelines) to maintain consistency with other WATcloud projects.

You are welcome to join the [WATcloud weekly meetings](/docs/community-docs/watcloud#meetings) to discuss your project and get feedback from the team.

## Projects

If you can complete one of the projects below, you are guaranteed a spot on the team!

import { Callout } from 'nextra/components'
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert"
import { CheckSquareIcon } from "lucide-react"


### Automated Power Outage Notification System

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/3371 */}

Create a system to automate notifications for power outages impacting our server room (CPH, 3rd floor).
The system should monitor the PlantOps [Service Interruptions page](https://plantops.uwaterloo.ca/service-interruptions/) to detect relevant updates and notify cluster users as needed.
Notifications should include the date, time, and expected duration of the outage.
We will also add additional information to the notification after the MVP, such as the impact on the cluster and any actions users should take.
Here's a list of historical announcements [1](https://github.com/WATonomous/infrastructure-support/discussions), [2](https://groups.google.com/a/watonomous.ca/g/watcloud-compute-cluster-announcements).

#### Requirements
- The tool should process historical outage data to demonstrate sufficiently low false positive and false negative rates.
- Inputs to the tool can be the website or any equivalent data source (e.g., email). The tool is responsible for parsing the natural language and generating announcements when necessary.

The deployment of this tool (mechanism to send notifications) can be done with the WATcloud team when the tool is ready for deployment.


### WATcloud CLI

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/3003 */}

As a WATcloud compute cluster user, there are a few things that you might want to do frequently, such as:
- Checking your quota usage (disk, CPU, memory)
- Checking the status of user daemons like Docker rootless
- Checking the status of the cluster (whether nodes are up, whether the cluster is under maintenance)

All of these can be done in user space (no special privileges required) with commands documented in various places in [our documentation](/docs/compute-cluster).
However, it can be tedious to run multiple commands to get this information.
We would like to have a CLI (command-line interface) tool that can provide all of this information in a single command.

Here are some initial commands the CLI tool should have:
- `watcloud status`: Get the status of the cluster, show machines that are up/down and whether they are under maintenance.
- `watcloud quota list`: List the quota usage of the user. Can be expanded in the future to submit quota edit requests.
- `watcloud daemon status`: Get the status of user daemons like Docker rootless.

### Grammar and Style Checker

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/1700 */}

We have a lot of documentation, and we want to make sure that it is easy to read and understand.
We would like to have a tool that can check our documentation for grammar and style issues.
This tool should detect capitalization issues, such as those outlined in the [guidelines](/docs/community-docs/watcloud/guidelines#communicate-accurately).
Additionally, it would be nice if this tool can check for common grammar mistakes, such as subject-verb agreement, punctuation, and sentence structure.
This tool can be run as a part of our CI/CD pipeline and should be used to check all documentation and, optionally, code comments.


### Open Source Tickets

Some of our open-source projects have open tickets that anyone can work on.
Here's a collection of tickets that we think are suitable for new contributors:

- [ ] [Support displaying job dependencies via flows](https://github.com/WATonomous/github-actions-tracing/issues/12) ([GitHub Actions Tracing](https://github.com/WATonomous/github-actions-tracing))
- [ ] [Support Dark Mode](https://github.com/WATonomous/watcloud-emails/issues/17) ([WATcloud Emails](https://github.com/WATonomous/watcloud-emails/))
- [ ] [Use SMTP pool to send emails](https://github.com/WATonomous/terraform-provider-email/issues/12)

### File Auto-Expiration Tool

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/1143 */}

<Callout type="info">
This project is currently in the deployment stage. The source code will be made available once deployment is complete.
</Callout>

At WATcloud, we have many shared drives that are used by our users to store files. Some drives, like the [scratch drive](../docs/compute-cluster/machine-usage-guide#mntscratch-directory), is meant for temporary storage. However, users often forget to delete their files, and drives quickly fills up. We need a tool that can give us a list of files that have not been accessed in a long time, so that we can take appropriate action (e.g. notify the user, then delete the file). This tool should be a lightweight script that we can run on a schedule.

Assume that the drive is 2-5 TiB, backed by NVMe SSD. The filesystem type is flexible, but preferably ext4 or xfs. The tool should have minimal impact on drive lifespan. Please be aware of the different timestamp types (e.g. access time, modification time, inode change time), and how they are accounted for by different filesystems and access methods.

### Automatic DNS failover

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/2541 */}

<Callout type="info">
This project is currently in the deployment stage. The source code will be made available once deployment is complete.
</Callout>

We host a Kubernetes cluster on our infrastructure and run a number of services. The services are exposed via [nginx-ingress](https://github.com/kubernetes/ingress-nginx). Different machines are assigned the same DNS name. For example, we could have `s3.watonomous.ca` point to all Kubernetes hosts in the cluster (using multiple DNS A records), and the client accessing `s3.watonomous.ca` would send requests to one of the hosts, and nginx-ingress would route the request to the appropriate service. This is a simple way to reduce downtime, since if one of the hosts goes down, there's only a `1/n` chance that the client will be affected[^assume-round-robin]. However, this is still not ideal. Most clients are not designed with a retry mechanism, and certainly rarer to have a retry mechanism that re-issues DNS lookups. We would like to have a tool that can automatically detect when a host goes down, and remove its DNS record from the DNS server. This way, clients will be less likely to be affected by a host going down.

We use Cloudflare as our DNS provider. Cloudflare was generous enough to give us a sponsorship that included [Zero-Downtime Failover](https://developers.cloudflare.com/fundamentals/basic-tasks/protect-your-origin-server/#zero-downtime-failover). This works well for externally-accessible services, but we also have internal services that resolve to IP addresses that are only accessible from the cluster. This tool will help us achieve a similar[^similar-reliability] level of reliability for internal services.

[^assume-round-robin]: We are assuming that there's perfect DNS round-robin or random selection.
[^similar-reliability]: It's slightly less reliable than Cloudflare's Zero-Downtime Failover, since there's a delay between when the host goes down and when the DNS record is removed. However, this is still much better than the current situation, where the DNS record of a downed host is never removed.


### Broken External Link Detector

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/2837 */}


<Alert variant="success" className="mt-6">
  <CheckSquareIcon className='h-5 w-5'/>
  <AlertTitle>Completed</AlertTitle>
  <AlertDescription>
    This project has been completed and the initial version is available [here](https://github.com/WATonomous/watcloud-website/commit/ec049baa33ea4e5b9d6820941db07c905c7b3af4)
    Please don't hesitate to reach out if you have any comments or suggestions!
  </AlertDescription>
</Alert>

In [broken internal link detector](#broken-internal-link-detector), we implemented a tool that can detect broken internal links in our website. We would like to extend this tool/develop a new tool that can detect broken external links.

Special considerations:
- What happens if the external link is down temporarily? We have no control over how long the link will be down.
- How do we handle links that require authentication? For example, on some member-facing pages, we have links to private GitHub repos. These links will return a 404 if the user is not authenticated. Should we simply whitelist these links, or is there a better way to handle them?

We don't know the answers to these questions, and we'd love to hear your thoughts!


### Terraform Provider Rate Limiting/Retry Mechanism

{/* Public reference: https://github.com/WATonomous/terraform-provider-email/issues/1 */}

<Alert variant="success" className="mt-6">
  <CheckSquareIcon className='h-5 w-5'/>
  <AlertTitle>Completed</AlertTitle>
  <AlertDescription>
    This project has been completed and the source code is available [here](https://github.com/WATonomous/terraform-provider-email/pull/2). Please don't hesitate to reach out if you have any comments or suggestions!
  </AlertDescription>
</Alert>

We use a [custom Terraform provider](https://github.com/WATonomous/terraform-provider-email/) for managing outgoing emails.
Currently, the provider is a simple wrapper around an SMTP client.
The SMTP server we use appears to have a rate limit.
It errors (SMTP 421) when we try to send more than a few emails in quick succession.

We would like to add rate limiting or a retry mechanism to the provider. [Here's the ticket for this feature](https://github.com/WATonomous/terraform-provider-email/issues/1).

### Blog Comment/Vote System

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/1663 */}

<Alert variant="success" className="mt-6">
  <CheckSquareIcon className='h-5 w-5'/>
  <AlertTitle>Completed</AlertTitle>
  <AlertDescription>
    This project has been completed and the initial version is available [here](https://github.com/WATonomous/watcloud-website/commit/624e95b330e0674c438df2f55f898d857f62dc6a).
    Please don't hesitate to reach out if you have any comments or suggestions!
  </AlertDescription>
</Alert>

As we prepare to launch the [WATcloud blog](/blog), we would like to integrate a comment/vote system that allows readers to engage with the content.
Some requirements for this system are:
- Easy to deploy and maintain: We want a system where almost all components can be automatically deployed (Infrastructure as Code).
- Minimal infrastructure requirements: We want to avoid running servers/databases if possible[^current-website].
- No paid subscriptions: We want to avoid services that require a paid subscription to use. This is because our funding does not allow for recurring costs.

[^current-website]: The current website is a statically-generated Next.js site, hosted on GitHub Pages. We would like to keep the infrastructure requirements similar to the current website.

Currently, we are considering the following options:

1. [**Giscus**](https://giscus.app/):
    - A lightweight commenting system that uses GitHub Discussions to manage and store comments.
    - Supports comments and reactions.
    - Examples: [1](https://the-guild.dev/blog/nextra-3), [2](https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/), [3]( https://jjtech.dev/reverse-engineering/imessage-explained/)

2. [**Utterances**](https://github.com/utterance/utterances):
    - A similar system to Giscus, but uses GitHub Issues to store comments.
    - Examples: [1](https://www.swyx.io/upload)

3. A simple like button:
    - Simple, but often involves running a (minimal) server/database or using an external service that may require a paid subscription.
    - Examples: [1](https://lyket.dev/blog/posts/gatsby-like-button)
    {/* [2](https://www.madeinsparetime.com/2022/08/06/making-a-like.html) */}

Some other options are described in these articles:
- https://darekkay.com/blog/static-site-comments/
- https://getshifter.io/static-site-comments/

The blog is a part of our website. The website source code can be found [here](https://github.com/WATonomous/watcloud-website).


### Broken Internal Link Detector

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/996#issuecomment-1875748581 */}

<Alert variant="success" className="mt-6">
  <CheckSquareIcon className='h-5 w-5'/>
  <AlertTitle>Completed</AlertTitle>
  <AlertDescription>
    This project has been completed and the initial version is available [here](https://github.com/WATonomous/watcloud-website/blob/9772e79a0d775ec80e7b2a3ac791d0e280ffef02/scripts/validate-internal-links.py).
    Please don't hesitate to reach out if you have any comments or suggestions!
  </AlertDescription>
</Alert>

We have a statically-generated Next.js website[^website]. Sometimes, we make typos in our hyperlinks. We would like to have a tool that can detect broken internal links. This should be a tool that runs at build-time and fails the build if it detects a broken link. The tool should be able to handle links to hashes (e.g. `#section`) in addition to links to pages. An initial brainstorm of how this could be implemented is available [here](https://chat.openai.com/share/0e0ffb40-1110-4bd5-8a1a-dd22a0e6483d).

[^website]: The source code of the website is accessible at https://github.com/WATonomous/watcloud-website

### Linux User Manager

{/* 
Internal references:
- https://github.com/WATonomous/infra-config/issues/960#issuecomment-1704040966
- https://github.com/WATonomous/infra-config/issues/1834
*/}

<Alert variant="success" className="mt-6">
  <CheckSquareIcon className='h-5 w-5'/>
  <AlertTitle>Completed</AlertTitle>
  <AlertDescription>
    This project has been completed and the source code is available [here](https://github.com/WATonomous/linux-directory-provisioner). Please don't hesitate to reach out if you have any comments or suggestions!
  </AlertDescription>
</Alert>

At WATcloud, we use [Ansible](https://www.ansible.com/) for provisioning machines and users. However, due to the nature of Ansible, there are a lot of inefficiencies in user provisioning. The provisioning pipeline's running time scales linearly with the number of users[^ansible-user-linear]. As of 2023, we have accumulated over 300 users in the cluster. This results in a single provisioning step that takes over 15 minutes. We would like to have a tool that can manage users on a machine, and that can be used in place of Ansible for user provisioning. This tool should be able to accept the following arguments:

- Managed UID range: the range of UIDs that the tool has control over
- Managed GID range: the range of GIDs that the tool has control over
- User list (username, UID, password, SSH keys): a list of users that the tool should manage.
- Group list (groupname, GID, members): a list of groups that the tool should manage.

[^ansible-user-linear]: Ansible issues a separate command for each action for each user. Even with the [pipelining](https://docs.ansible.com/ansible/latest/reference_appendices/config.html#ansible-pipelining) feature, the provisioning pipeline is unbearably slow.

### Azure Cost Estimator

{/* Internal reference: https://github.com/WATonomous/infra-config/issues/2057 */}

<Callout type="info">
This project is no longer needed because Azure has migrated our nonprofit subscription to a new model that includes cost management features.
</Callout>

We use an Azure nonprofit subscription for several projects, with an annual credit limit.
Tracking our usage effectively is challenging due to these limitations in Azure:
1. The Azure portal only displays current usage without projections or historical trends.
2. Access to the Azure sponsorship portal is restricted to a single user.

To better manage our resources, we need a tool that provides detailed insights into our Azure credit usage. The ideal tool would:
1. Display current usage and remaining credits.
2. Chart historical usage trends.
3. Project future usage based on past data.
4. Provide detailed breakdowns by resource for all the above metrics.

We are considering using [CAnalyzer](https://gitlab.com/indimin/canalyzer), but are open to any other suggestions.

If you don't have access to an Azure subscription, we can give you read-only access to our Azure portal.
Please fill out the [onboarding-form](/docs/utilities/onboarding-form) (make sure to enable the `Azure` section)
and [let us know](https://cloud.watonomous.ca/docs/compute-cluster/support-resources).
</file>

<file path="get-involved/sponsor.mdx">
# Sponsoring

Sponsorships help us maintain and expand the compute cluster. WATcloud started out on a shoestring budget. Over the years, we learned to be extremely efficient with our resources. We maximize the utility per dollar by designing our architecture around low-cost commodity hardware, aggressively seeking out opportune purchase windows (e.g. sales, promotions, bulk discounts, auctions), and using robust software for infrastructure provisioning and monitoring. For example, in 2023, we built a workstation machine with 6 GPUs (144 GiB VRAM in total), 512 GiB RAM, and 128 logical cores for less than CAD \$23,000[^trpro-cost]. An equivalent machine from a system integrator would cost at least CAD \$35,000 [^system-integrator-quote].

[^trpro-cost]: The cost of the machine is calculated based on the price of the components at the time of purchase. A rough breakdown is as follows:
    - 2x RTX 4090 GPU: around CAD \$4,800
    - 4x RTX 3090 GPU: around CAD \$6,000
    - Threadripper 3995WX CPU (tray): CAD \$4,650
    - 512 GiB RAM: CAD \$4,215
    - Motherboard, PSU, SSDs, etc.: around CAD \$3,000
    - Total: around CAD \$22,665

[^system-integrator-quote]: Based on quotes we received from popular system integrators, many of whom gave us an academic discount.

## Your impact

WATcloud is used by students and researchers at the University of Waterloo and partnering student organizations. Our users come from a wide range of backgrounds, including computer science, engineering, and mathematics. They use WATcloud to run simulations, train machine learning models, and perform data analysis for projects like autonomous driving, robotics, and computational biology. WATcloud is maintained by a team of volunteers, who also gain valuable experience in system administration, DevOps, automation, and cloud infrastructure. As of January 2025, over 650 students and researchers have benefited from WATcloud, producing numerous research papers and projects. By sponsoring WATcloud, you will be supporting the next generation of researchers and engineers.

## What you get

If you are a company, we will feature your name on our website and in our promotional materials. We will also mention your sponsorship in our social media posts. If your contribution is significant, we will also refer students from both WATcloud and our parent organization, [WATonomous](https://watonomous.ca), to your company for co-op and full-time positions. If you are an individual, we will discuss with you on how you would like to be recognized.

## How to sponsor

If you are interested in sponsoring WATcloud, please contact us at infra-outreach@watonomous.ca.

## Our sponsors

We would like to thank the following organizations and individuals for their generous support:

- [UWaterloo Math Endowment Fund (MEF)](https://uwaterloo.ca/math-endowment-fund/) for providing numerous rounds of funding to purchase hardware (parts of wato-wato1, wato-wato2, wato-wato3, wato-tr, and wato-thor were funded by MEF).
- [UWaterloo Engineering Endowment Fund (WEEF)](https://uwaterloo.ca/engineering-endowment-foundation/) for providing numerous rounds of funding to purchase hardware (parts of wato-delta, wato-tr, wato-thor, and wato-trpro were funded by WEEF).
- [UWaterloo Engineering Society (EngSoc)](https://engsoc.uwaterloo.ca) for providing funding to purchase networking equipment and other peripherals.
- [WATonomous](https://watonomous.ca) for providing the initial equipment for building the cluster (wato-wato1, wato-wato2, wato-wato3, wato-delta) and for continuing to provide a platform for WATcloud to grow[^wato-funding].
- [UWaterloo Engineering Computing](https://uwaterloo.ca/engineering-computing/) for providing a server room and equipment to host the cluster, as well as for installing and maintaining the network interface to the rest of the campus.
- [Prof. Derek Rayside](https://uwaterloo.ca/electrical-computer-engineering/profile/drayside) for contributing multiple machines and for providing advisory support.
- [1Password](https://1password.com) for providing an in-kind sponsorship of their password manager.
- [Teleport](https://goteleport.com) for providing an in-kind sponsorship of their infrastructure access product.
- [Cloudflare](https://cloudflare.com) for providing an in-kind sponsorship of their various products.
- [Sentry](https://sentry.io) for providing an in-kind sponsorship of their error monitoring product.
- [Elastic](https://elastic.co) for providing an in-kind sponsorship of their observability products.


[^wato-funding]: Almost all of WATcloud's funding is obtained as a part of WATonomous's funding efforts.
</file>

<file path="_app.mdx">
import '../styles/global.css';
import App from '@/components/_app_custom'

export default App
</file>

<file path="_document.tsx">
import { Html, Head, Main, NextScript } from 'next/document'
 
export default function Document() {
  return (
    <Html lang="en">
      <Head />
      <body>
        <Main />
        <NextScript />
      </body>
    </Html>
  )
}
</file>

<file path="_meta.json">
{
  "index": {
    "title": "Welcome",
    "type": "page",
    "display": "hidden",
    "theme": {
      "layout": "full",
      "timestamp": false
    }
  },
  "machines": {
    "title": "Machines",
    "type": "page",
    "theme": {
      "layout": "full",
      "timestamp": false
    }
  },
  "docs": {
    "title": "Documentation",
    "type": "page"
  },
  "blog": {
    "title": "Blog",
    "type": "page",
    "theme": {
      "typesetting": "article",
      "timestamp": false
    }
  },
  "get-involved": {
    "title": "Get Involved",
    "type": "page",
    "theme": {
      "timestamp": false
    }
  },
  "onboarding-form": {
    "display": "hidden"
  }
}
</file>

<file path="blog.mdx">
import { BlogHeader, BlogIndex, SubscribeDialog } from '@/components/blog'
import { Separator } from "@/components/ui/separator";

<BlogHeader />
<BlogIndex />
<SubscribeDialog />
</file>

<file path="index.mdx">
---
title: "Welcome"
---

import { Hero } from '@/components/hero'
 

<Hero />
</file>

<file path="machines.mdx">
# Machines

import { MachineCard } from '@/components/machine-card'
import { machineInfo } from '@/lib/data' 
import { pluralizeWithCount } from '@/lib/utils'

There are
[{pluralizeWithCount(machineInfo.machines['slurm_compute_nodes'].length, "SLURM compute node")}](#slurm-compute-nodes)
, [{pluralizeWithCount(machineInfo.machines['dev_vms'].length, "general-use machine")}](#general-use-machines)
, and [{pluralizeWithCount(machineInfo.machines['bastions'].length, "bastion host")}](#bastion-hosts)
hosted on [{pluralizeWithCount(machineInfo.machines['bare_metals'].length, "bare-metal machine")}](#bare-metal-machines)
in the cluster.

### SLURM Compute Nodes

The following machines are used to run SLURM jobs. To use them, please follow the [SLURM](/docs/compute-cluster/slurm) guide.

{
    <div className="items-start justify-center gap-6 rounded-lg p-8 grid md:grid-cols-2 xl:grid-cols-3">
      { machineInfo.machines['slurm_compute_nodes'].map((machine, index) => <MachineCard key={index} machine={machine} />) }
    </div>
}

### General-Use Machines

The following machines are available for general use. To access them, please follow the instructions in the [SSH](/docs/compute-cluster/ssh) guide.

{
    <div className="items-start justify-center gap-6 rounded-lg p-8 grid md:grid-cols-2 xl:grid-cols-3">
      { machineInfo.machines['dev_vms'].map((machine, index) => <MachineCard key={index} machine={machine} />) }
    </div>
}

### Bastion Hosts

The following machines are [bastions](/docs/compute-cluster/firewall#bastion). They can be used as [SSH jump hosts](https://wiki.gentoo.org/wiki/SSH_jump_host) to access the cluster from outside the university firewall.

{
    <div className="items-start justify-center gap-6 rounded-lg p-8 grid md:grid-cols-2 xl:grid-cols-3">
      { machineInfo.machines['bastions'].map((machine, index) => <MachineCard key={index} machine={machine} />) }
    </div>
}

### Bare-Metal Machines

The following machines are reserved for running supporting services (hypervisor, distributed filesystem, NAS, etc.) for the cluster.
They are accessible only to the cluster administrators.

{
    <div className="items-start justify-center gap-6 rounded-lg p-8 grid md:grid-cols-2 xl:grid-cols-3">
      { machineInfo.machines['bare_metals'].map((machine, index) => <MachineCard key={index} machine={machine} />) }
    </div>
}
</file>

<file path="onboarding-form.mdx">
# Onboarding Form (Moved)

import { INITIAL_FORM_DATA_QUERY_PARAM } from '@/components/onboarding-form'
import { encodeURI as b64EncodeURI } from 'js-base64';
import { Link } from 'nextra-theme-docs'

export function RedirectLink({ children }) {
    const urlParams = new URLSearchParams(typeof window === "undefined" ? "" : window.location.search);
    const initialFormDataStr = urlParams.get('initialFormData');

    const newURLParams = new URLSearchParams();
    if (initialFormDataStr) {
        newURLParams.append(INITIAL_FORM_DATA_QUERY_PARAM, b64EncodeURI(initialFormDataStr));
    }
    const newURLParamsStr = newURLParams.toString();

    return <Link href={`/docs/utilities/onboarding-form${newURLParamsStr ? `?${newURLParamsStr}` : ""}`}>{children}</Link>
}

This page has moved! Please <RedirectLink>click here</RedirectLink> to go to the new location.
</file>

</files>
